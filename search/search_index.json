{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"architectures/","title":"Available Model Architectures","text":"<p>If you would like your architecture added, please submit a pull request.</p>"},{"location":"architectures/#standard-2d","title":"Standard - 2D","text":"Architecture Module Description Based on FNO <code>ResNet</code> with <code>FourierBasicBlock</code> Fourier Neural Operator implementation with support for deeper architectures (8 and 4 layers) ResNet <code>ResNet</code> with <code>BasicBlock</code> ResNet architectures using 8 residual blocks, no downscaling DilResNet <code>ResNet</code> with <code>DilatedBasicBlock</code> ResNet where each block individually consists of 7 dilated CNN layers with dilation rates of [1, 2, 4, 8, 4, 2, 1], no downscaling U-Net<sub>2015</sub> <code>Unet2015</code> Original U-Net implementation U-Net<sub>base</sub> <code>Unetbase</code> Our interpretation of original U-Net implementation without bottleneck layer and using <code>GroupNorm</code> U-Net<sub>mod</sub> <code>Unet</code> Modern U-Nets with Wide ResNet blocks, as used in various diffusion modeling applications U-F[*]Net <code>FourierUnet</code> Modern U-Nets with [*]  Fourier blocks in the downsampling path UNO <code>UNO</code> Original U-shaped Neural Operator Implementation"},{"location":"architectures/#conditioned-2d","title":"Conditioned - 2D","text":"<p>Note</p> <p>Currently only scalar parameter conditioning is available.</p> Architecture Model Name Description FNO <code>ResNet</code> with <code>FourierBasicBlock</code> <code>Addition</code> based conditioning in both spatial and spectral domain. U-Net-modern <code>Unet</code> <code>Addition</code> and <code>AdaGN</code> style conditioning in the spatial domain. UF-Net <code>FourierUnet</code> <code>Addition</code> and <code>AdaGN</code> style conditioning in the spatial domain, <code>Addition</code> in the spectral domain."},{"location":"architectures/#maxwell-3d","title":"Maxwell - 3D","text":"Architecture Model Name Description Based on MaxwellFNO3D <code>MaxwellResNet3D</code> with <code>FourierBasicBlock3D</code> 3D Fourier Neural Operator implementation with support for deeper architectures (currently 4 layers) MaxwellCFNO3D <code>CliffordMaxwellResNet3D</code> with <code>CliffordFourierBasicBlock3D</code> 3D Clifford Fourier Neural Operator implementation with support for deeper architectures (currently 4 layers)"},{"location":"architectures/#model-architecture-registry-philosophy","title":"Model Architecture Registry Philosophy","text":"<p>While in principle we can make every architecture fully configurable via configuration files, we find it can affect the readability of the code quite a bit. Feel free to open issues or pull-requests for further configuration ability or any other suggestions for managing the configurability-readability tradeoffs.</p>"},{"location":"architectures/#other-projects-of-interest","title":"Other Projects of Interest","text":"<ul> <li>PDEBench</li> <li>NVIDIA Modulus</li> <li>NeuralOperators.jl</li> </ul>"},{"location":"data/","title":"Data Generation","text":"<p>Tip</p> <p>For multi-gpu training, make sure that the number of shards is divisible by the number of GPUs. 8 is usually a safe number.</p>"},{"location":"data/#navier-stokes-2d","title":"Navier Stokes 2D","text":""},{"location":"data/#standard","title":"Standard","text":"<pre><code># This script loops over different seeds and produces 5.2k training, 1.3k valid,\n# and 1.3k test trajectories of the Navier-Stokes dataset.\n./pdedatagen/scripts/navierstokes_jobs.sh\n</code></pre> <pre><code>#!/bin/bash\nseeds=(197910 452145 540788 61649 337293 296323 471551 238734 \\\n795028 806339 144207 415274 950547 391920 891493 645944 \\\n431652 391355 600690 495919 874847 97373 403121 588319 \\\n991570 597761 453345 349940 666497 597535 61891 975433 \\\n856942 788627 234361 433043 153164 126188 946091 795833 \\\n901348 142003 515976 509478 857366 766053 792079 585498 \\\n772145 954313 429673 445536)\nfor SEED in ${seeds[*]};\ndo\n    python scripts/generate_data.py base=pdedatagen/configs/navierstokes2dsmoke.yaml \\\n    experiment=smoke mode=train samples=100 seed=$SEED pdeconfig.init_args.sample_rate=4 \\\n    dirname=pdearena_data/navierstokes/\n    python scripts/generate_data.py base=pdedatagen/configs/navierstokes2dsmoke.yaml \\\n    experiment=smoke mode=valid samples=25 seed=$SEED pdeconfig.init_args.sample_rate=4 \\\n    dirname=pdearena_data/navierstokes/\n    python scripts/generate_data.py base=pdedatagen/configs/navierstokes2dsmoke.yaml \\\n    experiment=smoke mode=test samples=25 seed=$SEED pdeconfig.init_args.sample_rate=4 \\\n    dirname=pdearena_data/navierstokes/\ndone\n</code></pre>"},{"location":"data/#conditioned","title":"Conditioned","text":"<pre><code># This script loops over different seeds and produces 6656 training, 1664 valid,\n# and 1664 test trajectories of the conditioned Navier-Stokes dataset.\n./pdedatagen/scripts/navierstokes_cond_jobs.sh\n</code></pre> <pre><code>#!/bin/bash\nseeds=(197910 452145 540788 61649 337293 296323 471551 238734 \\\n795028 806339 144207 415274 950547 391920 891493 645944 \\\n431652 391355 600690 495919 874847 97373 403121 588319 \\\n991570 597761 453345 349940 666497 597535 61891 975433 \\\n856942 788627 234361 433043 153164 126188 946091 795833 \\\n901348 142003 515976 509478 857366 766053 792079 585498 \\\n772145 954313 429673 445536)\nfor SEED in ${seeds[*]};\ndo\n    python scripts/generate_data.py base=pdedatagen/configs/navierstokes2dsmoke.yaml \\\n    experiment=smoke_cond mode=train samples=128 seed=$SEED \\\n    dirname=pdearena_data/navierstokes_cond\n    python scripts/generate_data.py base=pdedatagen/configs/navierstokes2dsmoke.yaml \\\n    experiment=smoke_cond mode=valid samples=32 seed=$SEED \\\n    dirname=pdearena_data/navierstokes_cond\n    python scripts/generate_data.py base=pdedatagen/configs/navierstokes2dsmoke.yaml \\\n    experiment=smoke_cond mode=test samples=32 seed=$SEED \\\n    dirname=pdearena_data/navierstokes_cond\ndone\n</code></pre>"},{"location":"data/#data-normalization","title":"Data normalization","text":"<p>The Navier-Stokes data is reasonably bounded that we didn't need any normalization.</p>"},{"location":"data/#shallow-water-2d","title":"Shallow water 2D","text":"<pre><code># This script loops over different seeds and produces 5.6k training, 1.4k valid,\n# and 1.4k test trajectories of the Shallow water dataset.\n./pdedatagen/scripts/navierstokes_cond_jobs.sh\n</code></pre> <pre><code>#!/bin/bash\n# This script produces 5.6k training, 1.4k valid, and 1.4k test trajectories of the Shallow water dataset.\nseeds=(197910 452145 540788 61649 337293 296323 471551 238734 \\\n795028 806339 144207 415274 950547 391920 891493 645944 \\\n431652 391355 600690 495919 874847 97373 403121 588319 \\\n991570 597761 453345 349940 666497 597535 61891 975433 \\\n856942 788627 234361 433043 153164 126188 946091 795833 \\\n901348 142003 515976 509478 857366 766053 792079 585498 \\\n772145 954313 429673 445536 799432 146142 19024 438811)\nfor SEED in ${seeds[*]};\ndo\n    python scripts/generate_data.py base=pdedatagen/configs/shallowwater.yaml \\\n    experiment=shallowwater mode=train samples=100 seed=$SEED \\\n    dirname=pdearena_data/shallowwater;\n    python scripts/generate_data.py base=pdedatagen/configs/shallowwater.yaml \\\n    experiment=shallowwater mode=valid samples=25 seed=$SEED \\\n    dirname=pdearena_data/shallowwater;\n    python scripts/generate_data.py base=pdedatagen/configs/shallowwater.yaml \\\n    experiment=shallowwater mode=test samples=25 seed=$SEED \\\n    dirname=pdearena_data/shallowwater;\ndone;\n</code></pre>"},{"location":"data/#convert-to-zarr","title":"Convert to <code>zarr</code>","text":"<p>We found that data loading was a lot more performant with <code>zarr</code> format rather than original <code>NetCDF</code> format, especially with cloud storage. You can convert after data generation via:</p> <pre><code>for mode in train valid test; do\n    python scripts/convertnc2zarr.py \"pdearena_data/shallowwater/$mode\";\ndone\n</code></pre>"},{"location":"data/#data-normalization_1","title":"Data normalization","text":"<pre><code>python scripts/compute_normalization.py \\\n    --dataset shallowwater pdearena_data/shallowwater\n</code></pre>"},{"location":"data/#maxwell-3d","title":"Maxwell 3D","text":"<pre><code># This script loops over different seeds and produces 6.4k training,\n# 1.6k valid, and 1.6k test trajectories of the Maxwell dataset.\n</code></pre> <pre><code>#!/bin/bash\nseeds=(197910 452145 540788 61649 337293 296323 471551 238734 \\\n795028 806339 144207 415274 950547 391920 891493 645944 \\\n431652 391355 600690 495919 874847 97373 403121 588319 \\\n991570 597761 453345 349940 666497 597535 61891 975433 \\\n856942 788627 234361 433043 153164 126188 946091 795833 \\\n901348 142003 515976 509478 857366 766053 792079 585498 \\\n772145 954313 429673 445536 799432 146142 19024 438811 \\\n190539 506225 943948 304836 854174 354248 373230 697045)\nfor SEED in ${seeds[*]};\ndo\n    python scripts/generate_data.py base=pdedatagen/configs/maxwell3d.yaml \\\n    experiment=maxwell mode=train samples=100 seed=$SEED dirname=pdearena_data/maxwell3d/\n    python scripts/generate_data.py base=pdedatagen/configs/maxwell3d.yaml \\\n    experiment=maxwell mode=valid samples=25 seed=$SEED dirname=pdearena_data/maxwell3d/\n    python scripts/generate_data.py base=pdedatagen/configs/maxwell3d.yaml \\\n    experiment=maxwell mode=test samples=25 seed=$SEED dirname=pdearena_data/maxwell3d/\ndone\n</code></pre>"},{"location":"data/#data-normalization_2","title":"Data normalization","text":"<pre><code>python scripts/compute_normalization.py \\\n    --dataset maxwell pdearena_data/maxwell3d\n</code></pre>"},{"location":"data/#pdebench","title":"PDEBench","text":""},{"location":"data/#generating","title":"Generating","text":"<p>Follow PDEBench's instructions.</p>"},{"location":"data/#resharding-for-multi-gpu-experiments","title":"Resharding for multi-gpu experiments","text":"<p>Coming soon...</p>"},{"location":"data/#data-normalization_3","title":"Data normalization","text":"<p>Coming soon...</p>"},{"location":"data/#your-pde","title":"Your PDE","text":"<p>Please submit a pull request to add a data loading pipeline for your PDE dataset.</p>"},{"location":"datadownload/","title":"Downloading data from Huggingface","text":"<p>The data of PDEArena is currently hosted on Huggingface: https://huggingface.co/pdearena.</p> <p>Downloading the data is simple:</p>"},{"location":"datadownload/#navier-stokes-2d","title":"Navier Stokes - 2D","text":"<p>Generated using \u03a6<sub>Flow</sub>.</p>"},{"location":"datadownload/#standard-dataset","title":"Standard dataset","text":"<p>SSH: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone git@hf.co:datasets/pdearena/NavierStokes-2D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p> <p>HTTPS: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/datasets/pdearena/NavierStokes-2D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p>"},{"location":"datadownload/#conditioning-dataset","title":"Conditioning dataset","text":"<p>SSH: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone git@hf.co:datasets/pdearena/NavierStokes-2D-conditoned\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p> <p>HTTPS: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/datasets/pdearena/NavierStokes-2D-conditoned\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p>"},{"location":"datadownload/#shallow-water-2d","title":"Shallow water - 2D","text":"<p>Generated using SpeedyWeather.jl.</p> <p>SSH: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone git@hf.co:datasets/pdearena/ShallowWater-2D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p> <p>HTTPS: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/datasets/pdearena/ShallowWater-2D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p>"},{"location":"datadownload/#maxwell-3d","title":"Maxwell - 3D","text":"<p>Generated using Python 3D FDTD Simulator.</p> <p>SSH: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone git@hf.co:datasets/pdearena/Maxwell-3D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p> <p>HTTPS: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/datasets/pdearena/Maxwell-3D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p>"},{"location":"datadownload/#kuramoto-sivashinsky-1d","title":"Kuramoto-Sivashinsky - 1D","text":"<p>Generated using LPSDA.</p> <p>SSH: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone git@hf.co:datasets/pdearena/Kuramoto-Sivashinsky-1D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p> <p>HTTPS: <pre><code># Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/datasets/pdearena/Kuramoto-Sivashinsky-1D\n\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\nGIT_LFS_SKIP_SMUDGE=1\n</code></pre></p>"},{"location":"datadownload/#pdebench","title":"PDEBench","text":"<p><code>PDEBench</code> data can be downloaded with the included <code>download_pdebenchdata.py</code> script:</p> <p>For example to download the Incompressible Navier Stokes dataset:</p> <pre><code>export DATAVERSE_URL=https://darus.uni-stuttgart.de;\npython scripts/download_pdebenchdata.py \\\n    --outdir /mnt/data/PDEBench/ --limit ns_incom\n</code></pre>"},{"location":"install/","title":"Installation Guide","text":"clone the repo<pre><code>git clone https://github.com/microsoft/pdearena\n</code></pre> <code>conda</code><code>docker</code> create and activate env<pre><code>cd pdearena\nconda env create --file docker/environment.yml\nconda activate pdearena\n</code></pre> install this package<pre><code># install so the project is in PYTHONPATH\npip install -e .\n</code></pre> additionally installing Clifford Neural Layers<pre><code>pip install \"cliffordlayers @ git+https://github.com/microsoft/cliffordlayers\"\n</code></pre> <p>If you also want to do data generation:</p> <pre><code>pip install -e \".[datagen]\"\n</code></pre> build docker container<pre><code>cd docker\ndocker build -t pdearena .\n</code></pre> run docker container<pre><code>cd pdearena\ndocker run --gpus all -it --rm --user $(id -u):$(id -g) \\\n    -v $(pwd):/code -v /mnt/data:/data --workdir /code -e PYTHONPATH=/code \\\n    pdearena:latest\n</code></pre> <p>Note</p> <ul> <li><code>--gpus all -it --rm --user $(id -u):$(id -g)</code>: enables using all GPUs and runs an interactive session with current user's UID/GUID to avoid <code>docker</code> writing files as root.</li> <li><code>-v $(pwd):/code -v /mnt/data:/data --workdir /code</code>: mounts current directory and data directory (i.e. the cloned git repo) to <code>/code</code> and <code>/data</code> respectively, and use the <code>code</code> directory as the current working directory.</li> </ul>"},{"location":"modelzoo/","title":"Model Zoo","text":""},{"location":"modelzoo/#model-zoo","title":"Model Zoo","text":"<p>Here we document the performance of various models currently implemented with PDEArena. The tables below provide results and useful statistics about training and inference.</p> Model Name Num. Params Model Size (MB) Peak GPU Memory (MB) Eag. Fwd. Time (s) Comp. Fwd. Time (s) Forward+Backward Time (s) DilResNet128 <code>DilResNet-128</code> 4.2 M 16.7 4886.0 0.106 0.118 0.330 DilResNet-128-norm <code>DilResNet-128-norm</code> 4.2 M 16.7 6959.0 0.161 0.167 0.401 FNO128-8<sub>modes16</sub> <code>FNO-128-16m</code> 134 M 537.5 2964.01 0.042 0.042 0.129 FNO128-8<sub>modes8</sub> <code>FNO-128-8m</code> 33.7 M 134.9 2173.5 0.040 0.040 0.119 FNO128-4<sub>modes16</sub> <code>FNOs-128-16m</code> 67.2 M 268.8 1866.45 0.022 0.022 0.068 FNO128-4<sub>modes32</sub> <code>FNOs-128-32m</code> 268 M 1074.1 3434.51 0.027 0.027 0.097 FNO64-4<sub>modes32</sub> <code>FNOs-64-32m</code> 67.1 M 268.5 1225.52 0.012 0.012 0.040 FNO96-4<sub>modes32</sub> <code>FNOs-96-32m</code> 151 M 604.2 2195.01 0.020 0.020 0.064 ResNet128 <code>ResNet-128</code> 1.2 M 4.9 2500.03 0.040 0.043 0.090 ResNet256 <code>ResNet-256</code> 4.9 M 19.4 4958.0 0.112 0.101 0.254 U-F1Net<sub>modes16</sub> <code>U-FNet1-16m</code> 160 M 643.6 4054.3 0.083 0.156 0.197 U-F1Net<sub>modes16</sub>-1x1 <code>U-FNet1-16m-1x1</code> 160 M 643.6 4056.31 0.082 0.156 0.195 U-F1Net<sub>modes8</sub> <code>U-FNet1-8m</code> 148 M 593.3 3949.85 0.083 0.200 0.196 U-F1Net<sub>modes8</sub>-1x1 <code>U-FNet1-8m-1x1</code> 148 M 593.3 3953.76 0.082 0.168 0.194 U-F2Net<sub>modes16,8</sub> <code>U-FNet2-16m</code> 175 M 700.5 4172.36 0.084 0.093 0.201 U-F2Net<sub>modes16,8</sub>-1x1 <code>U-FNet2-16m-1x1</code> 175 M 700.5 4167.87 0.083 0.092 0.200 U-F2Net<sub>modes16,16</sub> <code>U-FNet2-16mc</code> 219 M 876.7 4514.14 0.085 0.092 0.206 U-F2Net<sub>modes8,4</sub> <code>U-FNet2-8m</code> 151 M 606.2 3984.61 0.083 0.145 0.199 U-F2Net<sub>modes8,4</sub>-1x1 <code>U-FNet2-8m-1x1</code> 151 M 606.1 3979.97 0.083 0.107 0.197 U-F2Net<sub>modes8,8</sub> <code>U-FNet2-8mc</code> 162 M 650.2 4064.52 0.084 0.099 0.200 U-F2Net<sub>att,modes16,8</sub> <code>U-FNet2attn-16m</code> 179 M 717.3 4247.5 0.086 0.168 0.208 U-F2Net<sub>att,modes16,8</sub>-1x1 <code>U-FNet2attn-16m-1x1</code> 179 M 717.3 4246.0 0.085 0.098 0.206 U-F3Net<sub>modes16,8,4</sub> <code>U-FNet3-16m</code> 187 M 751.9 4267.51 0.085 0.091 0.204 U-F3Net<sub>modes16,8,4</sub>-1x1 <code>U-FNet3-16m-1x1</code> 187 M 751.9 4271.79 0.084 0.090 0.202 U-F3Net<sub>modes8,4,2</sub> <code>U-FNet3-8m</code> 164 M 657.5 4080.25 0.084 0.092 0.202 U-F3Net<sub>modes8,4,2</sub>-1x1 <code>U-FNet3-8m-1x1</code> 164 M 657.5 4079.46 0.083 0.091 0.200 UNO128 <code>UNO-128</code> 440 M 1761.8 5525.0 0.152 0.155 0.342 UNO64 <code>UNO-64</code> 110 M 440.5 1946.17 0.062 0.066 0.135 U-Net<sub>2015</sub>128 <code>Unet2015-128</code> 124 M 496.5 3015.0 0.043 0.043 0.119 U-Net<sub>2015</sub>64 <code>Unet2015-64</code> 31.0 M 124.2 1357.08 0.013 0.013 0.038 U-Net<sub>base</sub>128 <code>Unetbase-128</code> 124 M 496.6 3016.0 0.055 0.044 0.135 U-Net<sub>base</sub>64 <code>Unetbase-64</code> 31.1 M 124.2 1291.83 0.020 0.013 0.046 U-Net<sub>mod</sub>64 <code>Unetmod-64</code> 144 M 577.1 3917.08 0.080 0.058 0.187 U-Net<sub>mod</sub>64-1x1 <code>Unetmod-64-1x1</code> 144 M 577.0 3917.95 0.080 0.057 0.185 U-Net<sub>att</sub>64 <code>Unetmodattn-64</code> 148 M 593.9 3991.47 0.082 0.060 0.193 U-Net<sub>att</sub>64-1x1 <code>Unetmodattn-64-1x1</code> 148 M 593.8 3994.42 0.082 0.059 0.191 MaxwellFNO3D~96-1x1 <code>MaxwellFNO3D</code> 55 M MaxwellCFNO3D~32-1x1 <code>MaxwellCFNO3D</code> 55 M <ul> <li>Date Created: 2023-03-27 01:42:54.528237</li> <li>GPU: Tesla V100-PCIE-16GB</li> </ul>"},{"location":"repomap/","title":"Repository Map","text":"<pre><code>\ud83d\udcc1 pdearena/\n    \ud83d\udcc1 data/ # (1)\n        \ud83d\udcc1 twod/ # (2)\n            \ud83d\udcc1 datapipes/\n                \ud83d\udcc4 common.py # (3)\n                ... # (4)\n    \ud83d\udcc1 models/\n        \ud83d\udcc4 registry.py # (5)\n        \ud83d\udcc4 pdemodel.py # (6)\n        \ud83d\udcc4 cond_pdemode.py # (7)\n    \ud83d\udcc1 modules/ #(8)\n        \ud83d\udcc1 conditioned/\n            ...\n        \ud83d\udcc4 twod_resnet.py\n        \ud83d\udcc4 twod_unet2015.py\n        \ud83d\udcc4 twod_unetbase.py\n        \ud83d\udcc4 twod_unet.py\n        \ud83d\udcc4 twod_uno.py\n        \ud83d\udcc4 activations.py # (9)\n        \ud83d\udcc4 loss.py # (10)\n        ...\n    \ud83d\udcc4 utils.py\n    ...\n\ud83d\udcc1 pdedatagen/\n    \ud83d\udcc1 configs/ #(11)\n    \ud83d\udcc1 shallowwater\n    \ud83d\udcc4 navier_stokes.py\n    \ud83d\udcc4 pde.py # (12)\n\ud83d\udcc1 scripts/\n    \ud83d\udcc4 train.py # (13)\n    \ud83d\udcc4 cond_train.py # (14)\n    \ud83d\udcc4 generate_data.py # (15)\n    \ud83d\udcc4 convertnc2zarr.py # (16)\n    \ud83d\udcc4 compute_normalization.py # (17)\n\ud83d\udcc1 configs/ # (18)\n</code></pre> <ol> <li>Everything data loading related goes here.</li> <li>Currently we only have 2D data support. But should be easy enough to add appropriate mechanisms for 1D, 3D and beyond.</li> <li>Common data pipe tranformations useful for building training and evaluation pipelines.</li> <li>Dataset opening data pipes for individual datasets.</li> <li>Model registry. Remember to register your new model here.</li> <li><code>LightningModule</code> to support standard PDE surrogate learning.</li> <li><code>LightningModule</code> to support time and parameter conditioned PDE surrogate learning.</li> <li>All the network architectures go here.</li> <li>Activation function registry</li> <li>Currently supported loss functions</li> <li>Configuration files for PDE data generation</li> <li>Register your new PDE configurations here</li> <li>Main training script for standard PDE surrogate training and testing</li> <li>Main training script for conditioned PDE surrogate training and testing</li> <li>Main script for generating data</li> <li>Supporting script to convert <code>netcdf</code> files to <code>zarr</code> for faster data loading</li> <li>Supporting script to compute the data normalization statistics. Add normalization methods for your data here.</li> <li>pytorch-lightning configs to run experiments.</li> </ol>"},{"location":"research/","title":"Research","text":""},{"location":"research/#research","title":"Research","text":"<p>Following is a list of research papers that have been published using the PDEArena framework.</p> <p>If you have used PDEArena in your research, and would like it listed here, please add your paper to this file by sending a pull request to the PDEArena repository.</p> <p></p> <p> Phillip Lippe <sup>1</sup>, Bastiaan S. Veeling <sup>1</sup>, Paris Perdikaris <sup>1</sup>, Richard E. Turner <sup>1</sup>, Johannes Brandstetter <sup>1</sup> <p><sup>1</sup>Microsoft Research AI4Science Abstract: Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.</p> <p></p> <p> David Ruhe <sup>1</sup>, Jayesh K. Gupta <sup>2</sup>, Steven de Keninck <sup>3</sup>, Max Welling <sup>1</sup>, Johannes Brandstetter <sup>1</sup> <p><sup>1</sup>Microsoft Research AI4Science, <sup>2</sup>Microsoft Autonomous Systems and Robotics Research, <sup>3</sup>University of Amsterdam Abstract: We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the Pin(p,q,r) group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable geometric templates that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.</p> <p></p> <p> Jayesh K. Gupta <sup>1</sup>, Johannes Brandstetter <sup>2</sup> <p><sup>1</sup>Microsoft Autonomous Systems and Robotics Research, <sup>2</sup>Microsoft Research AI4Science Abstract: Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. Various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local &amp; global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, generalizing across different equation parameters or time-scales still remains a challenge. In this work, we make a comprehensive comparison between various FNO, ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. We further analyze the design considerations for using FNO layers to improve performance of U-Net architectures without major degradation of computational cost. Finally, we show promising results on generalization to different PDE parameters and time-scales with a single surrogate model.</p> <p></p> <p> Johannes Brandstetter <sup>1</sup>, Rianne van den Berg <sup>1</sup>, Max Welling <sup>1</sup>, Jayesh K. Gupta <sup>2</sup> <p><sup>1</sup>Microsoft Research AI4Science, <sup>2</sup>Microsoft Autonomous Systems and Robotics Research Abstract: Partial differential equations (PDEs) see widespread use in sciences and engineering to describe simulation of physical processes as scalar and vector fields interacting and coevolving over time. Due to the computationally expensive nature of their standard solution methods, neural PDE surrogates have become an active research topic to accelerate these simulations. However, current methods do not explicitly take into account the relationship between different fields and their internal components, which are often correlated. Viewing the time evolution of such correlated fields through the lens of multivector fields allows us to overcome these limitations. Multivector fields consist of scalar, vector, as well as higher-order components, such as bivectors and trivectors. Their algebraic properties, such as multiplication, addition and other arithmetic operations can be described by Clifford algebras. To our knowledge, this paper presents the first usage of such multivector representations together with Clifford convolutions and Clifford Fourier transforms in the context of deep learning. The resulting Clifford neural layers are universally applicable and will find direct use in the areas of fluid dynamics, weather forecasting, and the modeling of physical systems in general. We empirically evaluate the benefit of Clifford neural layers by replacing convolution and Fourier operations in common neural PDE surrogates by their Clifford counterparts on two-dimensional Navier-Stokes and weather modeling tasks, as well as three-dimensional Maxwell equations. Clifford neural layers consistently improve generalization capabilities of the tested neural PDE surrogates.</p>"},{"location":"research/#pde-refiner-achieving-accurate-long-rollouts-with-neural-pde-solvers","title":"PDE-Refiner - Achieving Accurate Long Rollouts with Neural PDE Solvers","text":""},{"location":"research/#geometric-clifford-algebra-networks","title":"Geometric Clifford Algebra Networks","text":""},{"location":"research/#towards-multi-spatiotemporal-scale-generalized-pde-modeling","title":"Towards Multi-spatiotemporal-scale Generalized PDE Modeling","text":""},{"location":"research/#clifford-neural-layers-for-pde-modeling","title":"Clifford Neural Layers for PDE Modeling","text":""},{"location":"train/","title":"Training PDE surrogates","text":"<p>Thanks to PyTorch Lightning, whether it's a single GPU experiment or multiple GPUs (even multiple nodes), setting up scalable training experiments should be fairly simple.</p> <p>Tip</p> <p>We recommend a warmup learning rate schedule for distributed training.</p>"},{"location":"train/#standard-pde-surrogate-learning","title":"Standard PDE Surrogate Learning","text":"<pre><code>python scripts/train.py -c &lt;path/to/config&gt;\n</code></pre> <p>For example, to run modern U-Net on Navier Stokes dataset on 4 GPUs use:</p> <pre><code>python scripts/train.py -c configs/navierstokes2d.yaml \\\n    --data.data_dir=/mnt/data/NavierStokes2D_smoke \\\n    --trainer.strategy=ddp --trainer.devices=4 \\\n    --trainer.max_epochs=50 \\\n    --data.batch_size=8 \\\n    --data.time_gap=0 --data.time_history=4 --data.time_future=1 \\\n    --model.name=Unetmod-64 \\\n    --model.lr=2e-4 \\\n    --optimizer=AdamW --optimizer.lr=2e-4 --optimizer.weight_decay=1e-5 \\\n    --lr_scheduler=LinearWarmupCosineAnnealingLR \\\n    --lr_scheduler.warmup_epochs=5 \\\n    --lr_scheduler.max_epochs=50 --lr_scheduler.eta_min=1e-7\n</code></pre>"},{"location":"train/#conditioned-pde-surrogate-learning","title":"Conditioned PDE Surrogate Learning","text":"<pre><code>python scripts/cond_train.py -c &lt;path/to/config&gt;\n</code></pre> <p>For example, to run modern U-Net on Navier Stokes dataset on 4 GPUs use:</p> <pre><code>python scripts/cond_train.py -c configs/cond_navierstokes2d.yaml \\\n    --data.data_dir=/mnt/data/NavierStokes2D_cond_smoke_v1 \\\n    --trainer.strategy=ddp --trainer.devices=4 \\\n    --trainer.max_epochs=50 \\\n    --data.batch_size=8 \\\n    --model.name=Unetmod-64 \\\n    --model.lr=2e-4 \\\n    --optimizer=AdamW --optimizer.lr=2e-4 --optimizer.weight_decay=1e-5 \\\n    --lr_scheduler=LinearWarmupCosineAnnealingLR \\\n    --lr_scheduler.warmup_epochs=5 \\\n    --lr_scheduler.max_epochs=50 --lr_scheduler.eta_min=1e-7\n</code></pre>"},{"location":"train/#pde-surrogate-learning-with-pde-refiner","title":"PDE Surrogate Learning with PDE-Refiner","text":"<pre><code>python scripts/pderefiner_train.py -c &lt;path/to/config&gt;\n</code></pre> <p>For example, to run PDE-Refiner with modern U-Net on Kuramoto-Sivashinsky use:</p> <pre><code>python scripts/pderefiner_train.py -c configs/kuramotosivashinsky1d.yaml \\\n    --data.data_dir /mnt/data/KuramotoSivashinsky1D/ \\\n    --trainer.devices=1\n</code></pre>"},{"location":"train/#dataloading-philosophy","title":"Dataloading philosophy","text":"<ul> <li>Use modern <code>torchdata</code> iterable datapipes as they scale better with cloud storage.</li> <li>Use equally sized shards for simpler scaling with PyTorch DDP.</li> </ul>"},{"location":"reference/condmodules/","title":"Available conditioned PDE Surrogate Modules","text":""},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.AttentionBlock","title":"<code>AttentionBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Attention block This is similar to transformer multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>the number of channels in the input</p> required <code>n_heads</code> <code>int</code> <p>the number of heads in multi-head attention</p> <code>1</code> <code>d_k</code> <code>Optional[int]</code> <p>the number of dimensions in each head</p> <code>None</code> <code>n_groups</code> <code>int</code> <p>the number of groups for group normalization</p> <code>1</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class AttentionBlock(nn.Module):\n    \"\"\"Attention block This is similar to [transformer multi-head\n    attention](https://arxiv.org/abs/1706.03762).\n\n    Args:\n        n_channels: the number of channels in the input\n        n_heads:  the number of heads in multi-head attention\n        d_k: the number of dimensions in each head\n        n_groups: the number of groups for [group normalization][torch.nn.GroupNorm]\n\n    \"\"\"\n\n    def __init__(self, n_channels: int, n_heads: int = 1, d_k: Optional[int] = None, n_groups: int = 1):\n        \"\"\" \"\"\"\n        super().__init__()\n\n        # Default `d_k`\n        if d_k is None:\n            d_k = n_channels\n        # Normalization layer\n        self.norm = nn.GroupNorm(n_groups, n_channels)\n        # Projections for query, key and values\n        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n        # Linear layer for final transformation\n        self.output = nn.Linear(n_heads * d_k, n_channels)\n        # Scale for dot-product attention\n        self.scale = d_k**-0.5\n        #\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n    def forward(self, x: torch.Tensor):\n        # Get shape\n        batch_size, n_channels, height, width = x.shape\n        # Change `x` to shape `[batch_size, seq, n_channels]`\n        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        attn = torch.einsum(\"bihd,bjhd-&gt;bijh\", q, k) * self.scale\n        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = attn.softmax(dim=1)\n        # Multiply by values\n        res = torch.einsum(\"bijh,bjhd-&gt;bihd\", attn, v)\n        # Reshape to `[batch_size, seq, n_heads * d_k]`\n        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n        # Transform to `[batch_size, seq, n_channels]`\n        res = self.output(res)\n\n        # Add skip connection\n        res += x\n\n        # Change to shape `[batch_size, in_channels, height, width]`\n        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n        return res\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.AttentionBlock.__init__","title":"<code>__init__(n_channels, n_heads=1, d_k=None, n_groups=1)</code>","text":"Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>def __init__(self, n_channels: int, n_heads: int = 1, d_k: Optional[int] = None, n_groups: int = 1):\n    \"\"\" \"\"\"\n    super().__init__()\n\n    # Default `d_k`\n    if d_k is None:\n        d_k = n_channels\n    # Normalization layer\n    self.norm = nn.GroupNorm(n_groups, n_channels)\n    # Projections for query, key and values\n    self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n    # Linear layer for final transformation\n    self.output = nn.Linear(n_heads * d_k, n_channels)\n    # Scale for dot-product attention\n    self.scale = d_k**-0.5\n    #\n    self.n_heads = n_heads\n    self.d_k = d_k\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.DownBlock","title":"<code>DownBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Down block This combines <code>ResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the first half of U-Net at each resolution.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>cond_channels</code> <code>int</code> <p>Number of channels in the conditioning vector.</p> required <code>has_attn</code> <code>bool</code> <p>Whether to use attention block</p> <code>False</code> <code>activation</code> <code>Module</code> <p>Activation function</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>False</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>).</p> <code>False</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class DownBlock(ConditionedBlock):\n    \"\"\"Down block This combines `ResidualBlock` and `AttentionBlock`.\n\n    These are used in the first half of U-Net at each resolution.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        cond_channels (int): Number of channels in the conditioning vector.\n        has_attn (bool): Whether to use attention block\n        activation (nn.Module): Activation function\n        norm (bool): Whether to use normalization\n        use_scale_shift_norm (bool): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`).\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        cond_channels: int,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        self.res = ResidualBlock(\n            in_channels,\n            out_channels,\n            cond_channels,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n        x = self.res(x, emb)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.Downsample","title":"<code>Downsample</code>","text":"<p>             Bases: <code>Module</code></p> <p>Scale down the feature map by \\(\\frac{1}{2} \\times\\)</p> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class Downsample(nn.Module):\n    r\"\"\"Scale down the feature map by $\\frac{1}{2} \\times$\"\"\"\n\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor):\n        return self.conv(x)\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.FourierDownBlock","title":"<code>FourierDownBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Down block This combines <code>ResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the first half of U-Net at each resolution.</p> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class FourierDownBlock(ConditionedBlock):\n    \"\"\"Down block This combines `ResidualBlock` and `AttentionBlock`.\n\n    These are used in the first half of U-Net at each resolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        cond_channels: int,\n        modes1: int = 16,\n        modes2: int = 16,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        self.res = FourierResidualBlock(\n            in_channels,\n            out_channels,\n            cond_channels,\n            modes1=modes1,\n            modes2=modes2,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n        x = self.res(x, emb)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.FourierResidualBlock","title":"<code>FourierResidualBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Fourier Residual Block to be used in modern Unet architectures.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>cond_channels</code> <code>int</code> <p>Number of channels in the conditioning vector.</p> required <code>modes1</code> <code>int</code> <p>Number of modes in the first dimension.</p> <code>16</code> <code>modes2</code> <code>int</code> <p>Number of modes in the second dimension.</p> <code>16</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization.</p> <code>False</code> <code>n_groups</code> <code>int</code> <p>Number of groups for group normalization.</p> <code>1</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>).</p> <code>False</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class FourierResidualBlock(ConditionedBlock):\n    \"\"\"Fourier Residual Block to be used in modern Unet architectures.\n\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        cond_channels (int): Number of channels in the conditioning vector.\n        modes1 (int): Number of modes in the first dimension.\n        modes2 (int): Number of modes in the second dimension.\n        activation (str): Activation function to use.\n        norm (bool): Whether to use normalization.\n        n_groups (int): Number of groups for group normalization.\n        use_scale_shift_norm (bool): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`).\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        cond_channels: int,\n        modes1: int = 16,\n        modes2: int = 16,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        n_groups: int = 1,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        self.use_scale_shift_norm = use_scale_shift_norm\n        if activation == \"gelu\":\n            self.activation = nn.GELU()\n        elif activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"silu\":\n            self.activation = nn.SiLU()\n        else:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        self.modes1 = modes1\n        self.modes2 = modes2\n\n        self.fourier1 = SpectralConv2d(in_channels, out_channels, cond_channels, modes1=self.modes1, modes2=self.modes2)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, padding_mode=\"zeros\")\n        self.fourier2 = SpectralConv2d(\n            out_channels, out_channels, cond_channels, modes1=self.modes1, modes2=self.modes2\n        )\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, padding_mode=\"zeros\")\n        # self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n        # self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n        # If the number of input channels is not equal to the number of output channels we have to\n        # project the shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        else:\n            self.shortcut = nn.Identity()\n\n        if norm:\n            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n        else:\n            self.norm1 = nn.Identity()\n            self.norm2 = nn.Identity()\n\n        self.cond_emb = nn.Linear(cond_channels, 2 * out_channels if use_scale_shift_norm else out_channels)\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n        h = self.activation(self.norm1(x))\n\n        x1 = self.fourier1(h, emb)\n        x2 = self.conv1(h)\n        out = x1 + x2\n\n        emb_out = self.cond_emb(emb)\n        while len(emb_out.shape) &lt; len(h.shape):\n            emb_out = emb_out[..., None]\n\n        if self.use_scale_shift_norm:\n            scale, shift = torch.chunk(emb_out, 2, dim=1)\n            h = self.norm2(out) * (1 + scale) + shift  # where we do -1 or +1 doesn't matter\n            h = self.activation(h)\n            x1 = self.fourier2(h, emb)\n            x2 = self.conv2(h)\n        else:\n            out = out + emb_out\n            out = self.activation(self.norm2(out))\n            x1 = self.fourier2(out, emb)\n            x2 = self.conv2(out)\n\n        out = x1 + x2 + self.shortcut(x)\n        return out\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.FourierUnet","title":"<code>FourierUnet</code>","text":"<p>             Bases: <code>Module</code></p> <p>Unet with Fourier layers in early downsampling blocks.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the input.</p> required <code>time_future</code> <code>int</code> <p>Number of time steps in the output.</p> required <code>hidden_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> required <code>modes1</code> <code>int</code> <p>Number of Fourier modes to use in the first spatial dimension.</p> <code>12</code> <code>modes2</code> <code>int</code> <p>Number of Fourier modes to use in the second spatial dimension.</p> <code>12</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization.</p> <code>False</code> <code>ch_mults</code> <code>list</code> <p>List of integers to multiply the number of channels by at each resolution.</p> <code>(1, 2, 2, 4)</code> <code>is_attn</code> <code>list</code> <p>List of booleans indicating whether to use attention at each resolution.</p> <code>(False, False, False, False)</code> <code>mid_attn</code> <code>bool</code> <p>Whether to use attention in the middle block.</p> <code>False</code> <code>n_blocks</code> <code>int</code> <p>Number of blocks to use at each resolution.</p> <code>2</code> <code>n_fourier_layers</code> <code>int</code> <p>Number of early downsampling layers to use Fourier layers in.</p> <code>2</code> <code>mode_scaling</code> <code>bool</code> <p>Whether to scale the number of modes with resolution.</p> <code>True</code> <code>param_conditioning</code> <code>Optional[str]</code> <p>Type of conditioning to use. Defaults to None.</p> <code>None</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>). Defaults to False.</p> <code>False</code> <code>use1x1</code> <code>bool</code> <p>Whether to use 1x1 convolutions in the initial and final layer.</p> <code>False</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class FourierUnet(nn.Module):\n    \"\"\"Unet with Fourier layers in early downsampling blocks.\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps in the input.\n        time_future (int): Number of time steps in the output.\n        hidden_channels (int): Number of channels in the first layer.\n        activation (str): Activation function to use.\n        modes1 (int): Number of Fourier modes to use in the first spatial dimension.\n        modes2 (int): Number of Fourier modes to use in the second spatial dimension.\n        norm (bool): Whether to use normalization.\n        ch_mults (list): List of integers to multiply the number of channels by at each resolution.\n        is_attn (list): List of booleans indicating whether to use attention at each resolution.\n        mid_attn (bool): Whether to use attention in the middle block.\n        n_blocks (int): Number of blocks to use at each resolution.\n        n_fourier_layers (int): Number of early downsampling layers to use Fourier layers in.\n        mode_scaling (bool): Whether to scale the number of modes with resolution.\n        param_conditioning (Optional[str]): Type of conditioning to use. Defaults to None.\n        use_scale_shift_norm (bool): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`). Defaults to False.\n        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        activation: str,\n        modes1=12,\n        modes2=12,\n        norm: bool = False,\n        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n        mid_attn: bool = False,\n        n_blocks: int = 2,\n        n_fourier_layers: int = 2,\n        mode_scaling: bool = True,\n        param_conditioning: Optional[str] = None,\n        use_scale_shift_norm: bool = False,\n        use1x1: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.time_history = time_history\n        self.time_future = time_future\n        self.hidden_channels = hidden_channels\n\n        self.param_conditioning = param_conditioning\n        # Number of resolutions\n        n_resolutions = len(ch_mults)\n\n        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        n_channels = hidden_channels\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        time_embed_dim = hidden_channels * 4\n        self.time_embed = nn.Sequential(\n            nn.Linear(hidden_channels, time_embed_dim),\n            self.activation,\n            nn.Linear(time_embed_dim, time_embed_dim),\n        )\n        if self.param_conditioning is not None:\n            if self.param_conditioning == \"scalar\":\n                self.pde_emb = nn.Sequential(\n                    nn.Linear(hidden_channels, time_embed_dim),\n                    self.activation,\n                    nn.Linear(time_embed_dim, time_embed_dim),\n                )\n            else:\n                raise NotImplementedError(f\"param_conditioning {self.param_conditioning} not implemented\")\n        # Project image into feature map\n        if use1x1:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n        else:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # #### First half of U-Net - decreasing resolution\n        down = []\n        # Number of channels\n        out_channels = in_channels = n_channels\n        # For each resolution\n        for i in range(n_resolutions):\n            # Number of output channels at this resolution\n            out_channels = in_channels * ch_mults[i]\n            if i &lt; n_fourier_layers:\n                for _ in range(n_blocks):\n                    down.append(\n                        FourierDownBlock(\n                            in_channels,\n                            out_channels,\n                            time_embed_dim,\n                            modes1=max(modes1 // 2**i, 4) if mode_scaling else modes1,\n                            modes2=max(modes2 // 2**i, 4) if mode_scaling else modes2,\n                            has_attn=is_attn[i],\n                            activation=activation,\n                            norm=norm,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                        )\n                    )\n                    in_channels = out_channels\n            else:\n                # Add `n_blocks`\n                for _ in range(n_blocks):\n                    down.append(\n                        DownBlock(\n                            in_channels,\n                            out_channels,\n                            time_embed_dim,\n                            has_attn=is_attn[i],\n                            activation=activation,\n                            norm=norm,\n                        )\n                    )\n                    in_channels = out_channels\n            # Down sample at all resolutions except the last\n            if i &lt; n_resolutions - 1:\n                down.append(Downsample(in_channels))\n\n        # Combine the set of modules\n        self.down = nn.ModuleList(down)\n\n        # Middle block\n        self.middle = MiddleBlock(out_channels, time_embed_dim, has_attn=mid_attn, activation=activation, norm=norm)\n\n        # #### Second half of U-Net - increasing resolution\n        up = []\n        # Number of channels\n        in_channels = out_channels\n        # For each resolution\n        for i in reversed(range(n_resolutions)):\n            # `n_blocks` at the same resolution\n            out_channels = in_channels\n            for _ in range(n_blocks):\n                up.append(\n                    UpBlock(\n                        in_channels,\n                        out_channels,\n                        time_embed_dim,\n                        has_attn=is_attn[i],\n                        activation=activation,\n                        norm=norm,\n                    )\n                )\n            # Final block to reduce the number of channels\n            out_channels = in_channels // ch_mults[i]\n            up.append(\n                UpBlock(\n                    in_channels,\n                    out_channels,\n                    time_embed_dim,\n                    has_attn=is_attn[i],\n                    activation=activation,\n                    norm=norm,\n                )\n            )\n            in_channels = out_channels\n            # Up sample at all resolutions except last\n            if i &gt; 0:\n                up.append(Upsample(in_channels))\n\n        # Combine the set of modules\n        self.up = nn.ModuleList(up)\n\n        if norm:\n            self.norm = nn.GroupNorm(8, n_channels)\n        else:\n            self.norm = nn.Identity()\n        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n        if use1x1:\n            self.final = zero_module(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n        else:\n            self.final = zero_module(nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)))\n\n    def forward(self, x: torch.Tensor, time, z=None):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n\n        emb = self.time_embed(fourier_embedding(time, self.hidden_channels))\n        if z is not None:\n            if self.param_conditioning == \"scalar\":\n                emb = emb + self.pde_emb(fourier_embedding(z, self.hidden_channels))\n            else:\n                raise NotImplementedError(f\"param_conditioning {self.param_conditioning} not implemented\")\n\n        x = self.image_proj(x)\n\n        h = [x]\n        for m in self.down:\n            if isinstance(m, Downsample):\n                x = m(x)\n            else:\n                x = m(x, emb)\n            h.append(x)\n\n        x = self.middle(x, emb)\n\n        for m in self.up:\n            if isinstance(m, Upsample):\n                x = m(x)\n            else:\n                # Get the skip connection from first half of U-Net and concatenate\n                s = h.pop()\n                x = torch.cat((x, s), dim=1)\n                #\n                x = m(x, emb)\n\n        x = self.final(self.activation(self.norm(x)))\n        return x.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.FourierUpBlock","title":"<code>FourierUpBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Up block This combines <code>ResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the second half of U-Net at each resolution.</p> Note <p>We currently don't recommend using this block.</p> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class FourierUpBlock(ConditionedBlock):\n    \"\"\"Up block This combines `ResidualBlock` and `AttentionBlock`.\n\n    These are used in the second half of U-Net at each resolution.\n\n    Note:\n        We currently don't recommend using this block.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        cond_channels: int,\n        modes1: int = 16,\n        modes2: int = 16,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n        # from the first half of the U-Net\n        self.res = FourierResidualBlock(\n            in_channels + out_channels,\n            out_channels,\n            cond_channels,\n            modes1=modes1,\n            modes2=modes2,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n        x = self.res(x, emb)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.MiddleBlock","title":"<code>MiddleBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Middle block It combines a <code>ResidualBlock</code>, <code>AttentionBlock</code>, followed by another <code>ResidualBlock</code>.</p> <p>This block is applied at the lowest resolution of the U-Net.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of channels in the input and output.</p> required <code>cond_channels</code> <code>int</code> <p>Number of channels in the conditioning vector.</p> required <code>has_attn</code> <code>bool</code> <p>Whether to use attention block. Defaults to False.</p> <code>False</code> <code>activation</code> <code>str</code> <p>Activation function to use. Defaults to \"gelu\".</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization. Defaults to False.</p> <code>False</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>). Defaults to False.</p> <code>False</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class MiddleBlock(ConditionedBlock):\n    \"\"\"Middle block It combines a `ResidualBlock`, `AttentionBlock`, followed by another\n    `ResidualBlock`.\n\n    This block is applied at the lowest resolution of the U-Net.\n\n    Args:\n        n_channels (int): Number of channels in the input and output.\n        cond_channels (int): Number of channels in the conditioning vector.\n        has_attn (bool, optional): Whether to use attention block. Defaults to False.\n        activation (str): Activation function to use. Defaults to \"gelu\".\n        norm (bool, optional): Whether to use normalization. Defaults to False.\n        use_scale_shift_norm (bool, optional): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`). Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_channels: int,\n        cond_channels: int,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        self.res1 = ResidualBlock(\n            n_channels,\n            n_channels,\n            cond_channels,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n        self.attn = AttentionBlock(n_channels) if has_attn else nn.Identity()\n        self.res2 = ResidualBlock(\n            n_channels,\n            n_channels,\n            cond_channels,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor) -&gt; torch.Tensor:\n        x = self.res1(x, emb)\n        x = self.attn(x)\n        x = self.res2(x, emb)\n        return x\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.ResidualBlock","title":"<code>ResidualBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Wide Residual Blocks used in modern Unet architectures.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>cond_channels</code> <code>int</code> <p>Number of channels in the conditioning vector.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization.</p> <code>False</code> <code>n_groups</code> <code>int</code> <p>Number of groups for group normalization.</p> <code>1</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>).</p> <code>False</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class ResidualBlock(ConditionedBlock):\n    \"\"\"Wide Residual Blocks used in modern Unet architectures.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        cond_channels (int): Number of channels in the conditioning vector.\n        activation (str): Activation function to use.\n        norm (bool): Whether to use normalization.\n        n_groups (int): Number of groups for group normalization.\n        use_scale_shift_norm (bool): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`).\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        cond_channels: int,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        n_groups: int = 1,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        self.use_scale_shift_norm = use_scale_shift_norm\n        if activation == \"gelu\":\n            self.activation = nn.GELU()\n        elif activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"silu\":\n            self.activation = nn.SiLU()\n        else:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n        self.conv2 = zero_module(nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)))\n        # If the number of input channels is not equal to the number of output channels we have to\n        # project the shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        else:\n            self.shortcut = nn.Identity()\n\n        if norm:\n            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n        else:\n            self.norm1 = nn.Identity()\n            self.norm2 = nn.Identity()\n\n        self.cond_emb = nn.Linear(cond_channels, 2 * out_channels if use_scale_shift_norm else out_channels)\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n        # First convolution layer\n        h = self.conv1(self.activation(self.norm1(x)))\n        emb_out = self.cond_emb(emb)\n        while len(emb_out.shape) &lt; len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            scale, shift = torch.chunk(emb_out, 2, dim=1)\n            h = self.norm2(h) * (1 + scale) + shift  # where we do -1 or +1 doesn't matter\n            h = self.conv2(self.activation(h))\n        else:\n            h = h + emb_out\n            # Second convolution layer\n            h = self.conv2(self.activation(self.norm2(h)))\n        # Add the shortcut connection and return\n        return h + self.shortcut(x)\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.Unet","title":"<code>Unet</code>","text":"<p>             Bases: <code>Module</code></p> <p>Modern U-Net architecture</p> <p>This is a modern U-Net architecture with wide-residual blocks and spatial attention blocks</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the input</p> required <code>time_future</code> <code>int</code> <p>Number of time steps in the output</p> required <code>hidden_channels</code> <code>int</code> <p>Number of channels in the hidden layers</p> required <code>activation</code> <code>str</code> <p>Activation function to use</p> required <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>False</code> <code>ch_mults</code> <code>list</code> <p>List of channel multipliers for each resolution</p> <code>(1, 2, 2, 4)</code> <code>is_attn</code> <code>list</code> <p>List of booleans indicating whether to use attention blocks</p> <code>(False, False, False, False)</code> <code>mid_attn</code> <code>bool</code> <p>Whether to use attention block in the middle block</p> <code>False</code> <code>n_blocks</code> <code>int</code> <p>Number of residual blocks in each resolution</p> <code>2</code> <code>param_conditioning</code> <code>Optional[str]</code> <p>Type of conditioning to use. Defaults to None.</p> <code>None</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>). Defaults to False.</p> <code>False</code> <code>use1x1</code> <code>bool</code> <p>Whether to use 1x1 convolutions in the initial and final layers</p> <code>False</code> Note <p>Currently, only <code>scalar</code> parameter conditioning is supported.</p> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class Unet(nn.Module):\n    \"\"\"Modern U-Net architecture\n\n    This is a modern U-Net architecture with wide-residual blocks and spatial attention blocks\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps in the input\n        time_future (int): Number of time steps in the output\n        hidden_channels (int): Number of channels in the hidden layers\n        activation (str): Activation function to use\n        norm (bool): Whether to use normalization\n        ch_mults (list): List of channel multipliers for each resolution\n        is_attn (list): List of booleans indicating whether to use attention blocks\n        mid_attn (bool): Whether to use attention block in the middle block\n        n_blocks (int): Number of residual blocks in each resolution\n        param_conditioning (Optional[str]): Type of conditioning to use. Defaults to None.\n        use_scale_shift_norm (bool): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`). Defaults to False.\n        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layers\n\n    Note:\n        Currently, only `scalar` parameter conditioning is supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history,\n        time_future,\n        hidden_channels,\n        activation,\n        norm: bool = False,\n        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n        mid_attn: bool = False,\n        n_blocks: int = 2,\n        param_conditioning: Optional[str] = None,\n        use_scale_shift_norm: bool = False,\n        use1x1: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.time_history = time_history\n        self.time_future = time_future\n        self.hidden_channels = hidden_channels\n        self.activation = activation\n        self.param_conditioning = param_conditioning\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        # Number of resolutions\n        n_resolutions = len(ch_mults)\n\n        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        n_channels = hidden_channels\n        time_embed_dim = hidden_channels * 4\n        self.time_embed = nn.Sequential(\n            nn.Linear(hidden_channels, time_embed_dim),\n            self.activation,\n            nn.Linear(time_embed_dim, time_embed_dim),\n        )\n        if self.param_conditioning is not None:\n            if self.param_conditioning == \"scalar\":\n                self.pde_emb = nn.Sequential(\n                    nn.Linear(hidden_channels, time_embed_dim),\n                    self.activation,\n                    nn.Linear(time_embed_dim, time_embed_dim),\n                )\n            else:\n                raise NotImplementedError(f\"Param conditioning {self.param_conditioning} not implemented\")\n\n        # Project image into feature map\n        if use1x1:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n        else:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # #### First half of U-Net - decreasing resolution\n        down = []\n        # Number of channels\n        out_channels = in_channels = n_channels\n        # For each resolution\n        for i in range(n_resolutions):\n            # Number of output channels at this resolution\n            out_channels = in_channels * ch_mults[i]\n            # Add `n_blocks`\n            for _ in range(n_blocks):\n                down.append(\n                    DownBlock(\n                        in_channels,\n                        out_channels,\n                        time_embed_dim,\n                        has_attn=is_attn[i],\n                        activation=activation,\n                        norm=norm,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                )\n                in_channels = out_channels\n            # Down sample at all resolutions except the last\n            if i &lt; n_resolutions - 1:\n                down.append(Downsample(in_channels))\n\n        # Combine the set of modules\n        self.down = nn.ModuleList(down)\n\n        # Middle block\n        self.middle = MiddleBlock(\n            out_channels,\n            time_embed_dim,\n            has_attn=mid_attn,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n\n        # #### Second half of U-Net - increasing resolution\n        up = []\n        # Number of channels\n        in_channels = out_channels\n        # For each resolution\n        for i in reversed(range(n_resolutions)):\n            # `n_blocks` at the same resolution\n            out_channels = in_channels\n            for _ in range(n_blocks):\n                up.append(\n                    UpBlock(\n                        in_channels,\n                        out_channels,\n                        time_embed_dim,\n                        has_attn=is_attn[i],\n                        activation=activation,\n                        norm=norm,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                )\n            # Final block to reduce the number of channels\n            out_channels = in_channels // ch_mults[i]\n            up.append(\n                UpBlock(\n                    in_channels,\n                    out_channels,\n                    time_embed_dim,\n                    has_attn=is_attn[i],\n                    activation=activation,\n                    norm=norm,\n                    use_scale_shift_norm=use_scale_shift_norm,\n                )\n            )\n            in_channels = out_channels\n            # Up sample at all resolutions except last\n            if i &gt; 0:\n                up.append(Upsample(in_channels))\n\n        # Combine the set of modules\n        self.up = nn.ModuleList(up)\n\n        if norm:\n            self.norm = nn.GroupNorm(8, n_channels)\n        else:\n            self.norm = nn.Identity()\n        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n        if use1x1:\n            self.final = zero_module(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n        else:\n            self.final = zero_module(nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)))\n\n    def forward(self, x: torch.Tensor, time, z=None):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n\n        emb = self.time_embed(fourier_embedding(time, self.hidden_channels))\n        if z is not None:\n            if self.param_conditioning == \"scalar\":\n                emb = emb + self.pde_emb(fourier_embedding(z, self.hidden_channels))\n            else:\n                raise NotImplementedError(f\"Param conditioning {self.param_conditioning} not implemented\")\n\n        x = self.image_proj(x)\n\n        h = [x]\n        for m in self.down:\n            if isinstance(m, Downsample):\n                x = m(x)\n            else:\n                x = m(x, emb)\n            h.append(x)\n\n        x = self.middle(x, emb)\n\n        for m in self.up:\n            if isinstance(m, Upsample):\n                x = m(x)\n            else:\n                # Get the skip connection from first half of U-Net and concatenate\n                s = h.pop()\n                x = torch.cat((x, s), dim=1)\n                #\n                x = m(x, emb)\n\n        x = self.final(self.activation(self.norm(x)))\n        return x.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.UpBlock","title":"<code>UpBlock</code>","text":"<p>             Bases: <code>ConditionedBlock</code></p> <p>Up block This combines <code>ResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the second half of U-Net at each resolution.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>cond_channels</code> <code>int</code> <p>Number of channels in the conditioning vector.</p> required <code>has_attn</code> <code>bool</code> <p>Whether to use attention block</p> <code>False</code> <code>activation</code> <code>str</code> <p>Activation function</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>False</code> <code>use_scale_shift_norm</code> <code>bool</code> <p>Whether to use scale and shift approach to conditoning (also termed as <code>AdaGN</code>).</p> <code>False</code> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class UpBlock(ConditionedBlock):\n    \"\"\"Up block This combines `ResidualBlock` and `AttentionBlock`.\n\n    These are used in the second half of U-Net at each resolution.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        cond_channels (int): Number of channels in the conditioning vector.\n        has_attn (bool): Whether to use attention block\n        activation (str): Activation function\n        norm (bool): Whether to use normalization\n        use_scale_shift_norm (bool): Whether to use scale and shift approach to conditoning (also termed as `AdaGN`).\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        cond_channels: int,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        use_scale_shift_norm: bool = False,\n    ):\n        super().__init__()\n        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n        # from the first half of the U-Net\n        self.res = ResidualBlock(\n            in_channels + out_channels,\n            out_channels,\n            cond_channels,\n            activation=activation,\n            norm=norm,\n            use_scale_shift_norm=use_scale_shift_norm,\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor) -&gt; torch.Tensor:\n        x = self.res(x, emb)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.twod_unet.Upsample","title":"<code>Upsample</code>","text":"<p>             Bases: <code>Module</code></p> <p>Scale up the feature map by \\(2 \\times\\)</p> Source code in <code>pdearena/modules/conditioned/twod_unet.py</code> <pre><code>class Upsample(nn.Module):\n    r\"\"\"Scale up the feature map by $2 \\times$\"\"\"\n\n    def __init__(self, n_channels: int):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor):\n        return self.conv(x)\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.condition_utils.ConditionedBlock","title":"<code>ConditionedBlock</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>pdearena/modules/conditioned/condition_utils.py</code> <pre><code>class ConditionedBlock(nn.Module):\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"Apply the module to `x` given `emb` embdding of time or others.\"\"\"\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.condition_utils.ConditionedBlock.forward","title":"<code>forward(x, emb)</code>  <code>abstractmethod</code>","text":"<p>Apply the module to <code>x</code> given <code>emb</code> embdding of time or others.</p> Source code in <code>pdearena/modules/conditioned/condition_utils.py</code> <pre><code>@abstractmethod\ndef forward(self, x, emb):\n    \"\"\"Apply the module to `x` given `emb` embdding of time or others.\"\"\"\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.condition_utils.fourier_embedding","title":"<code>fourier_embedding(timesteps, dim, max_period=10000)</code>","text":"<p>Create sinusoidal timestep embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>timesteps</code> <code>Tensor</code> <p>a 1-D Tensor of N indices, one per batch element.           These may be fractional.</p> required <code>dim</code> <code>int</code> <p>the dimension of the output.</p> required <code>max_period</code> <code>int</code> <p>controls the minimum frequency of the embeddings.</p> <code>10000</code> <p>Returns:     embedding (torch.Tensor): [N \\(\\times\\) dim] Tensor of positional embeddings.</p> Source code in <code>pdearena/modules/conditioned/condition_utils.py</code> <pre><code>def fourier_embedding(timesteps: torch.Tensor, dim, max_period=10000):\n    r\"\"\"Create sinusoidal timestep embeddings.\n\n    Args:\n        timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n        dim (int): the dimension of the output.\n        max_period (int): controls the minimum frequency of the embeddings.\n    Returns:\n        embedding (torch.Tensor): [N $\\times$ dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"reference/condmodules/#pdearena.modules.conditioned.condition_utils.zero_module","title":"<code>zero_module(module)</code>","text":"<p>Zero out the parameters of a module and return it.</p> Source code in <code>pdearena/modules/conditioned/condition_utils.py</code> <pre><code>def zero_module(module):\n    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n</code></pre>"},{"location":"reference/dataload/","title":"DataModules","text":""},{"location":"reference/dataload/#pdearena.data.datamodule","title":"<code>pdearena.data.datamodule</code>","text":""},{"location":"reference/dataload/#pdearena.data.datamodule.PDEDataModule","title":"<code>PDEDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>Defines the standard dataloading process for PDE data.</p> <p>Does not support generalization to different parameterizations or time. Consider using pdearena.data.cond_datamodule.CondPDEDataModule for that.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The task to be solved.</p> required <code>data_dir</code> <code>str</code> <p>The path to the data directory.</p> required <code>time_history</code> <code>int</code> <p>The number of time steps in the past.</p> required <code>time_future</code> <code>int</code> <p>The number of time steps in the future.</p> required <code>time_gap</code> <code>int</code> <p>The number of time steps between the past and the future to be skipped.</p> required <code>pde</code> <code>dict</code> <p>The PDE to be solved.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> required <code>num_workers</code> <code>int</code> <p>The number of workers. Make sure when using values greater than 1 on multi-GPU systems, the number of shards is divisible by the number of workers times number of GPUs.</p> required <code>train_limit_trajectories</code> <code>int</code> <p>The number of trajectories to be used for training. This is from each shard.</p> required <code>valid_limit_trajectories</code> <code>int</code> <p>The number of trajectories to be used for validation. This is from each shard.</p> required <code>test_limit_trajectories</code> <code>int</code> <p>The number of trajectories to be used for testing. This is from each shard.</p> required <code>usegrid</code> <code>bool</code> <p>Whether to use a grid. Defaults to False.</p> <code>False</code> Source code in <code>pdearena/data/datamodule.py</code> <pre><code>class PDEDataModule(LightningDataModule):\n    \"\"\"Defines the standard dataloading process for PDE data.\n\n    Does not support generalization to different parameterizations or time.\n    Consider using [pdearena.data.cond_datamodule.CondPDEDataModule][] for that.\n\n    Args:\n        task (str): The task to be solved.\n        data_dir (str): The path to the data directory.\n        time_history (int): The number of time steps in the past.\n        time_future (int): The number of time steps in the future.\n        time_gap (int): The number of time steps between the past and the future to be skipped.\n        pde (dict): The PDE to be solved.\n        batch_size (int): The batch size.\n        pin_memory (bool): Whether to pin memory.\n        num_workers (int): The number of workers. Make sure when using values greater than 1 on multi-GPU systems, the number of shards is divisible by the number of workers times number of GPUs.\n        train_limit_trajectories (int): The number of trajectories to be used for training. This is from each shard.\n        valid_limit_trajectories (int): The number of trajectories to be used for validation. This is from each shard.\n        test_limit_trajectories (int): The number of trajectories to be used for testing. This is from each shard.\n        usegrid (bool, optional): Whether to use a grid. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        data_dir: str,\n        time_history: int,\n        time_future: int,\n        time_gap: int,\n        pde: PDEDataConfig,\n        batch_size: int,\n        pin_memory: bool,\n        num_workers: int,\n        train_limit_trajectories: int,\n        valid_limit_trajectories: int,\n        test_limit_trajectories: int,\n        usegrid: bool = False,\n    ):\n        super().__init__()\n        self.data_dir = data_dir\n        self.pde = pde\n\n        self.save_hyperparameters(ignore=\"pde\", logger=False)\n\n    def setup(self, stage: Optional[str] = None):\n        dps = DATAPIPE_REGISTRY[self.hparams.task]\n        self.train_dp = dps[\"train\"](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.train_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=self.hparams.time_history,\n            time_future=self.hparams.time_future,\n            time_gap=self.hparams.time_gap,\n        )\n        self.valid_dp1 = dps[\"valid\"][0](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.valid_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=self.hparams.time_history,\n            time_future=self.hparams.time_future,\n            time_gap=self.hparams.time_gap,\n        )\n        self.valid_dp2 = dps[\"valid\"][1](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.valid_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=self.hparams.time_history,\n            time_future=self.hparams.time_future,\n            time_gap=self.hparams.time_gap,\n        )\n        self.test_dp_onestep = dps[\"test\"][0](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.test_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=self.hparams.time_history,\n            time_future=self.hparams.time_future,\n            time_gap=self.hparams.time_gap,\n        )\n        self.test_dp = dps[\"test\"][1](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.test_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=self.hparams.time_history,\n            time_future=self.hparams.time_future,\n            time_gap=self.hparams.time_gap,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dp,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,\n            shuffle=True,\n            drop_last=True,\n            collate_fn=collate_fn_cat,\n        )\n\n    def val_dataloader(self):\n        timestep_loader = DataLoader(\n            dataset=self.valid_dp1,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn_cat,\n        )\n        rollout_loader = DataLoader(\n            dataset=self.valid_dp2,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,  # TODO: might need to reduce this\n            shuffle=False,\n            collate_fn=collate_fn_stack,\n        )\n        return [timestep_loader, rollout_loader]\n\n    def test_dataloader(self):\n        rollout_loader = DataLoader(\n            dataset=self.test_dp,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn_stack,\n        )\n        timestep_loader = DataLoader(\n            dataset=self.test_dp_onestep,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn_cat,\n        )\n        return [timestep_loader, rollout_loader]\n</code></pre>"},{"location":"reference/dataload/#pdearena.data.cond_datamodule","title":"<code>pdearena.data.cond_datamodule</code>","text":""},{"location":"reference/dataload/#pdearena.data.cond_datamodule.CondPDEDataModule","title":"<code>CondPDEDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>Definest the dataloading process for conditioned PDE data.</p> <p>Supports generalization experiments.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Name of the task.</p> required <code>data_dir</code> <code>str</code> <p>Path to the data directory.</p> required <code>pde</code> <code>dict</code> <p>Dictionary containing the PDE class and its arguments.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers.</p> required <code>train_limit_trajectories</code> <code>int</code> <p>Number of trajectories to use for training.</p> required <code>valid_limit_trajectories</code> <code>int</code> <p>Number of trajectories to use for validation.</p> required <code>test_limit_trajectories</code> <code>int</code> <p>Number of trajectories to use for testing.</p> required <code>eval_dts</code> <code>List[int]</code> <p>List of timesteps to use for evaluation. Defaults to [1, 2, 4, 8, 16].</p> <code>[1, 2, 4, 8, 16]</code> <code>usegrid</code> <code>bool</code> <p>Whether to use the grid. Defaults to False.</p> <code>False</code> Source code in <code>pdearena/data/cond_datamodule.py</code> <pre><code>class CondPDEDataModule(LightningDataModule):\n    \"\"\"Definest the dataloading process for conditioned PDE data.\n\n    Supports generalization experiments.\n\n    Args:\n        task (str): Name of the task.\n        data_dir (str): Path to the data directory.\n        pde (dict): Dictionary containing the PDE class and its arguments.\n        batch_size (int): Batch size.\n        pin_memory (bool): Whether to pin memory.\n        num_workers (int): Number of workers.\n        train_limit_trajectories (int): Number of trajectories to use for training.\n        valid_limit_trajectories (int): Number of trajectories to use for validation.\n        test_limit_trajectories (int): Number of trajectories to use for testing.\n        eval_dts (List[int], optional): List of timesteps to use for evaluation. Defaults to [1, 2, 4, 8, 16].\n        usegrid (bool, optional): Whether to use the grid. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        data_dir: str,\n        pde: PDEDataConfig,\n        batch_size: int,\n        pin_memory: bool,\n        num_workers: int,\n        train_limit_trajectories: int,\n        valid_limit_trajectories: int,\n        test_limit_trajectories: int,\n        eval_dts: List[int] = [1, 2, 4, 8, 16],\n        usegrid: bool = False,\n    ):\n        super().__init__()\n        self.data_dir = data_dir\n        self.eval_dts = eval_dts\n        self.pde = pde\n        self.save_hyperparameters(ignore=\"pde\", logger=False)\n\n        # if \"Weather\" in pde[\"class_path\"]:\n        #     self.dataset_opener = WeatherDatasetOpener\n        #     self.randomized_traindatapipe = RandomTimeStepPDETrainData\n        #     self.evaldatapipe = TimestepPDEEvalData\n        #     # self.train_filter = _weathertrain_filter\n        #     # self.valid_filter = _weathervalid_filter\n        #     # self.test_filter = _weathertest_filter\n        #     self.lister = lambda x: dp.iter.IterableWrapper(\n        #         map(lambda y: os.path.join(self.data_dir, y), os.listdir(x))\n        #     )\n        #     self.sharder = lambda x: x\n        # elif len(self.pde.grid_size) == 3:\n        #     self.dataset_opener = NavierStokesDatasetOpener\n        #     self.randomized_traindatapipe = RandomTimeStepPDETrainData\n        #     self.evaldatapipe = TimestepPDEEvalData\n        #     self.train_filter = _train_filter\n        #     self.valid_filter = _valid_filter\n        #     self.test_filter = _test_filter\n        #     self.lister = dp.iter.FileLister\n        #     self.sharder = dp.iter.ShardingFilter\n        # else:\n        #     raise NotImplementedError()\n\n    def setup(self, stage=None):\n        dps = DATAPIPE_REGISTRY[self.hparams.task]\n        self.train_dp = dps[\"train\"](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.train_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=1,\n            time_future=1,\n            time_gap=0,\n        )\n        self.valid_dps = [\n            dps[\"valid\"](\n                pde=self.pde,\n                data_path=self.data_dir,\n                limit_trajectories=self.hparams.valid_limit_trajectories,\n                usegrid=False,\n                time_history=1,\n                time_future=1,\n                time_gap=0,\n                delta_t=dt,\n            )\n            for dt in self.eval_dts\n        ]\n\n        self.test_dp = dps[\"test\"][1](\n            pde=self.pde,\n            data_path=self.data_dir,\n            limit_trajectories=self.hparams.test_limit_trajectories,\n            usegrid=self.hparams.usegrid,\n            time_history=1,\n            time_future=1,\n            time_gap=0,\n        )\n        self.test_dps = [\n            dps[\"test\"][0](\n                pde=self.pde,\n                data_path=self.data_dir,\n                limit_trajectories=self.hparams.test_limit_trajectories,\n                usegrid=False,\n                time_history=1,\n                time_future=1,\n                time_gap=0,\n                delta_t=dt,\n            )\n            for dt in self.eval_dts\n        ]\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dp,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,\n            shuffle=True,\n            drop_last=True,\n            collate_fn=collate_fn_cat,\n        )\n\n    def val_dataloader(self):\n        timestep_loaders = [\n            DataLoader(\n                dataset=dp,\n                num_workers=self.hparams.num_workers,\n                pin_memory=self.hparams.pin_memory,\n                batch_size=self.hparams.batch_size,\n                shuffle=False,\n                collate_fn=collate_fn_cat,\n            )\n            for dp in self.valid_dps\n        ]\n        return timestep_loaders\n\n    def test_dataloader(self):\n        rollout_loader = DataLoader(\n            dataset=self.test_dp,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            batch_size=self.hparams.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn_stack,\n        )\n        timestep_loader = [\n            DataLoader(\n                dataset=dp,\n                num_workers=self.hparams.num_workers,\n                pin_memory=self.hparams.pin_memory,\n                batch_size=self.hparams.batch_size,\n                shuffle=False,\n                collate_fn=collate_fn_cat,\n            )\n            for dp in self.test_dps\n        ]\n        return [rollout_loader] + timestep_loader\n</code></pre>"},{"location":"reference/datapipes/","title":"Datapipes","text":""},{"location":"reference/datapipes/#pdearena.data.datapipes_common","title":"<code>pdearena.data.datapipes_common</code>","text":""},{"location":"reference/datapipes/#pdearena.data.datapipes_common.RandomTimeStepConditionedPDETrainData","title":"<code>RandomTimeStepConditionedPDETrainData</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>Randomized data for training conditioned PDEs.</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>IterDataPipe</code> <p>Data pipe that returns individual PDE trajectories.</p> required <code>n_input_scalar_components</code> <code>int</code> <p>Number of input scalar components.</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of input vector components.</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components.</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components.</p> required <code>trajlen</code> <code>int</code> <p>Length of a trajectory in the dataset.</p> required <code>reweigh</code> <code>bool</code> <p>Whether to rebalance the dataset so that longer horizon predictions get equal weightage despite there being fewer actual such datapoints in a trajectory. Defaults to True.</p> <code>True</code> Source code in <code>pdearena/data/datapipes_common.py</code> <pre><code>class RandomTimeStepConditionedPDETrainData(dp.iter.IterDataPipe):\n    \"\"\"Randomized data for training conditioned PDEs.\n\n    Args:\n        dp (IterDataPipe): Data pipe that returns individual PDE trajectories.\n        n_input_scalar_components (int): Number of input scalar components.\n        n_input_vector_components (int): Number of input vector components.\n        n_output_scalar_components (int): Number of output scalar components.\n        n_output_vector_components (int): Number of output vector components.\n        trajlen (int): Length of a trajectory in the dataset.\n        reweigh (bool, optional): Whether to rebalance the dataset so that longer horizon predictions get equal weightage despite there being fewer actual such datapoints in a trajectory. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        dp,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        trajlen: int,\n        reweigh=True,\n    ) -&gt; None:\n        super().__init__()\n        self.dp = dp\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n\n        self.trajlen = trajlen\n        self.reweigh = reweigh\n\n    def __iter__(self):\n        time_resolution = self.trajlen\n\n        for u, v, cond, grid in self.dp:\n            if self.reweigh:\n                end_time = random.choices(range(1, time_resolution), k=1)[0]\n                start_time = random.choices(range(0, end_time), weights=1 / np.arange(1, end_time + 1), k=1)[0]\n            else:\n                end_time = torch.randint(low=1, high=time_resolution, size=(1,), dtype=torch.long).item()\n                start_time = torch.randint(low=0, high=end_time.item(), size=(1,), dtype=torch.long).item()\n\n            delta_t = end_time - start_time\n            yield (\n                *datautils.create_time_conditioned_data(\n                    self.n_input_scalar_components,\n                    self.n_input_vector_components,\n                    self.n_output_scalar_components,\n                    self.n_output_vector_components,\n                    u,\n                    v,\n                    grid,\n                    start_time,\n                    end_time,\n                    torch.tensor([delta_t]),\n                ),\n                cond,\n            )\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.datapipes_common.RandomizedPDETrainData","title":"<code>RandomizedPDETrainData</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>Randomized data for training PDEs.</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>IterDataPipe</code> <p>Data pipe that returns individual PDE trajectories.</p> required <code>n_input_scalar_components</code> <code>int</code> <p>Number of input scalar components.</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of input vector components.</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components.</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components.</p> required <code>trajlen</code> <code>int</code> <p>Length of a trajectory in the dataset.</p> required <code>time_history</code> <code>int</code> <p>Number of time steps of inputs.</p> required <code>time_future</code> <code>int</code> <p>Number of time steps of outputs.</p> required <code>time_gap</code> <code>int</code> <p>Number of time steps between inputs and outputs.</p> required Source code in <code>pdearena/data/datapipes_common.py</code> <pre><code>class RandomizedPDETrainData(dp.iter.IterDataPipe):\n    \"\"\"Randomized data for training PDEs.\n\n    Args:\n        dp (IterDataPipe): Data pipe that returns individual PDE trajectories.\n        n_input_scalar_components (int): Number of input scalar components.\n        n_input_vector_components (int): Number of input vector components.\n        n_output_scalar_components (int): Number of output scalar components.\n        n_output_vector_components (int): Number of output vector components.\n        trajlen (int): Length of a trajectory in the dataset.\n        time_history (int): Number of time steps of inputs.\n        time_future (int): Number of time steps of outputs.\n        time_gap (int): Number of time steps between inputs and outputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        dp,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        trajlen: int,\n        time_history: int,\n        time_future: int,\n        time_gap: int,\n    ) -&gt; None:\n        super().__init__()\n        self.dp = dp\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.trajlen = trajlen\n        self.time_history = time_history\n        self.time_future = time_future\n        self.time_gap = time_gap\n\n    def __iter__(self):\n        for batch in self.dp:\n            if len(batch) == 3:\n                (u, v, grid) = batch\n                cond = None\n            elif len(batch) == 4:\n                (u, v, cond, grid) = batch\n            else:\n                raise ValueError(f\"Unknown batch length of {len(batch)}.\")\n\n            # Length of trajectory\n            time_resolution = min(u.shape[0], self.trajlen)\n            # Max number of previous points solver can eat\n            reduced_time_resolution = time_resolution - self.time_history\n            # Number of future points to predict\n            max_start_time = reduced_time_resolution - self.time_future - self.time_gap\n\n            # Choose initial random time point at the PDE solution manifold\n            start_time = random.choices([t for t in range(max_start_time + 1)], k=1)\n            data, targets = datautils.create_data2D(\n                self.n_input_scalar_components,\n                self.n_input_vector_components,\n                self.n_output_scalar_components,\n                self.n_output_vector_components,\n                u,\n                v,\n                grid,\n                start_time[0],\n                self.time_history,\n                self.time_future,\n                self.time_gap,\n            )\n            if cond is None and grid is None:\n                yield data, targets\n            elif cond is not None and grid is None:\n                yield data, targets, cond\n            else:\n                yield data, targets, cond, grid\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.datapipes_common.TimestepConditionedPDEEvalData","title":"<code>TimestepConditionedPDEEvalData</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>Data for evaluation of time conditioned PDEs</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>IterDataPipe</code> <p>Data pipe that returns individual PDE trajectories.</p> required <code>trajlen</code> <code>int</code> <p>Length of a trajectory in the dataset.</p> required <code>delta_t</code> <code>int</code> <p>Evaluates predictions conditioned at that delta_t.</p> required Tip <p>Make sure <code>delta_t</code> is less than half of <code>trajlen</code>.</p> Source code in <code>pdearena/data/datapipes_common.py</code> <pre><code>class TimestepConditionedPDEEvalData(dp.iter.IterDataPipe):\n    \"\"\"Data for evaluation of time conditioned PDEs\n\n    Args:\n        dp (torchdata.datapipes.iter.IterDataPipe): Data pipe that returns individual PDE trajectories.\n        trajlen (int): Length of a trajectory in the dataset.\n        delta_t (int): Evaluates predictions conditioned at that delta_t.\n\n    Tip:\n        Make sure `delta_t` is less than half of `trajlen`.\n    \"\"\"\n\n    def __init__(self, dp: dp.iter.IterDataPipe, trajlen: int, delta_t: int) -&gt; None:\n        super().__init__()\n        self.dp = dp\n        self.trajlen = trajlen\n        if 2 * delta_t &gt;= self.trajlen:\n            raise ValueError(\"delta_t should be less than half the trajectory length\")\n\n        self.delta_t = delta_t\n\n    def __iter__(self):\n        for begin in range(self.trajlen - self.delta_t):\n            for u, v, cond, grid in self.dp:\n                newu = u[begin :: self.delta_t, ...]\n                newv = v[begin :: self.delta_t, ...]\n                max_start_time = newu.size(0)\n                for start in range(max_start_time - 1):\n                    end = start + 1\n                    data = torch.cat((newu[start : start + 1], newv[start : start + 1]), dim=1).unsqueeze(0)\n                    if grid is not None:\n                        data = torch.cat((data, grid), dim=1)\n                    label = torch.cat((newu[end : end + 1], newv[end : end + 1]), dim=1).unsqueeze(0)\n                    if data.size(1) == 0:\n                        raise ValueError(\"Data is empty. Likely indexing issue.\")\n                    if label.size(1) == 0:\n                        raise ValueError(\"Label is empty. Likely indexing issue.\")\n                    yield data, label, torch.tensor([self.delta_t]), cond\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.datapipes_common.ZarrLister","title":"<code>ZarrLister</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>Customized lister for zarr files.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Union[str, Sequence[str], IterDataPipe]</code> <p>Root directory. Defaults to \".\".</p> <code>'.'</code> <p>Yields:</p> Type Description <code>str</code> <p>Path to the zarr file.</p> Source code in <code>pdearena/data/datapipes_common.py</code> <pre><code>class ZarrLister(dp.iter.IterDataPipe):\n    \"\"\"Customized lister for zarr files.\n\n    Args:\n        root (Union[str, Sequence[str], dp.iter.IterDataPipe], optional): Root directory. Defaults to \".\".\n\n    Yields:\n        (str): Path to the zarr file.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: Union[str, Sequence[str], dp.iter.IterDataPipe] = \".\",\n    ) -&gt; None:\n        super().__init__()\n\n        if isinstance(root, str):\n            root = [root]\n        if not isinstance(root, dp.iter.IterDataPipe):\n            root = dp.iter.IterableWrapper(root)\n\n        self.datapipe: dp.iter.IterDataPipe = root\n\n    def __iter__(self):\n        for path in self.datapipe:\n            for dirname in os.listdir(path):\n                if dirname.endswith(\".zarr\"):\n                    yield os.path.join(path, dirname)\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.datapipes_common.build_datapipes","title":"<code>build_datapipes(pde, data_path, limit_trajectories, usegrid, dataset_opener, lister, sharder, filter_fn, mode, time_history=1, time_future=1, time_gap=0, onestep=False, conditioned=False, delta_t=None, conditioned_reweigh=True)</code>","text":"<p>Build datapipes for training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>pde</code> <code>PDEDataConfig</code> <p>PDE configuration.</p> required <code>data_path</code> <code>str</code> <p>Path to the data.</p> required <code>limit_trajectories</code> <code>int</code> <p>Number of trajectories to use.</p> required <code>usegrid</code> <code>bool</code> <p>Whether to use spatial grid as input.</p> required <code>dataset_opener</code> <code>Callable[..., IterDataPipe]</code> <p>Dataset opener.</p> required <code>lister</code> <code>Callable[..., IterDataPipe]</code> <p>List files.</p> required <code>sharder</code> <code>Callable[..., IterDataPipe]</code> <p>Shard files.</p> required <code>filter_fn</code> <code>Callable[..., IterDataPipe]</code> <p>Filter files.</p> required <code>mode</code> <code>str</code> <p>Mode of the data. [\"train\", \"valid\", \"test\"]</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the past. Defaults to 1.</p> <code>1</code> <code>time_future</code> <code>int</code> <p>Number of time steps in the future. Defaults to 1.</p> <code>1</code> <code>time_gap</code> <code>int</code> <p>Number of time steps between the past and the future to be skipped. Defaults to 0.</p> <code>0</code> <code>onestep</code> <code>bool</code> <p>Whether to use one-step prediction. Defaults to False.</p> <code>False</code> <code>conditioned</code> <code>bool</code> <p>Whether to use conditioned data. Defaults to False.</p> <code>False</code> <code>delta_t</code> <code>Optional[int]</code> <p>Time step size. Defaults to None. Only used for conditioned data.</p> <code>None</code> <code>conditioned_reweigh</code> <code>bool</code> <p>Whether to reweight conditioned data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dpipe</code> <code>IterDataPipe</code> <p>IterDataPipe for training and evaluation.</p> Source code in <code>pdearena/data/datapipes_common.py</code> <pre><code>def build_datapipes(\n    pde: PDEDataConfig,\n    data_path,\n    limit_trajectories,\n    usegrid: bool,\n    dataset_opener: Callable[..., dp.iter.IterDataPipe],\n    lister: Callable[..., dp.iter.IterDataPipe],\n    sharder: Callable[..., dp.iter.IterDataPipe],\n    filter_fn: Callable[..., dp.iter.IterDataPipe],\n    mode: str,\n    time_history=1,\n    time_future=1,\n    time_gap=0,\n    onestep=False,\n    conditioned=False,\n    delta_t: Optional[int] = None,\n    conditioned_reweigh: bool = True,\n):\n    \"\"\"Build datapipes for training and evaluation.\n\n    Args:\n        pde (PDEDataConfig): PDE configuration.\n        data_path (str): Path to the data.\n        limit_trajectories (int): Number of trajectories to use.\n        usegrid (bool): Whether to use spatial grid as input.\n        dataset_opener (Callable[..., dp.iter.IterDataPipe]): Dataset opener.\n        lister (Callable[..., dp.iter.IterDataPipe]): List files.\n        sharder (Callable[..., dp.iter.IterDataPipe]): Shard files.\n        filter_fn (Callable[..., dp.iter.IterDataPipe]): Filter files.\n        mode (str): Mode of the data. [\"train\", \"valid\", \"test\"]\n        time_history (int, optional): Number of time steps in the past. Defaults to 1.\n        time_future (int, optional): Number of time steps in the future. Defaults to 1.\n        time_gap (int, optional): Number of time steps between the past and the future to be skipped. Defaults to 0.\n        onestep (bool, optional): Whether to use one-step prediction. Defaults to False.\n        conditioned (bool, optional): Whether to use conditioned data. Defaults to False.\n        delta_t (Optional[int], optional): Time step size. Defaults to None. Only used for conditioned data.\n        conditioned_reweigh (bool, optional): Whether to reweight conditioned data. Defaults to True.\n\n    Returns:\n        dpipe (IterDataPipe): IterDataPipe for training and evaluation.\n    \"\"\"\n    dpipe = lister(\n        data_path,\n    ).filter(filter_fn=filter_fn)\n    if mode == \"train\":\n        dpipe = dpipe.shuffle()\n\n    dpipe = dataset_opener(\n        sharder(dpipe),\n        mode=mode,\n        limit_trajectories=limit_trajectories,\n        usegrid=usegrid,\n    )\n    if mode == \"train\":\n        # Make sure that in expectation we have seen all the data despite randomization\n        dpipe = dpipe.cycle(pde.trajlen)\n\n    if mode == \"train\":\n        # Training data is randomized\n        if conditioned:\n            dpipe = RandomTimeStepConditionedPDETrainData(\n                dpipe,\n                pde.n_scalar_components,\n                pde.n_vector_components,\n                pde.n_scalar_components,\n                pde.n_vector_components,\n                pde.trajlen,\n                conditioned_reweigh,\n            )\n        else:\n            dpipe = RandomizedPDETrainData(\n                dpipe,\n                pde.n_scalar_components,\n                pde.n_vector_components,\n                pde.n_scalar_components,\n                pde.n_vector_components,\n                pde.trajlen,\n                time_history,\n                time_future,\n                time_gap,\n            )\n    else:\n        # Evaluation data is not randomized.\n        if conditioned and onestep:\n            assert delta_t is not None\n            dpipe = TimestepConditionedPDEEvalData(dpipe, pde.trajlen, delta_t)\n        elif onestep:\n            dpipe = PDEEvalTimeStepData(\n                dpipe,\n                pde.n_scalar_components,\n                pde.n_vector_components,\n                pde.n_scalar_components,\n                pde.n_vector_components,\n                pde.trajlen,\n                time_history,\n                time_future,\n                time_gap,\n            )\n        # For multi-step prediction, the original data pipe can be used without change.\n\n    return dpipe\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.oned.datapipes.kuramotosivashinsky1d","title":"<code>pdearena.data.oned.datapipes.kuramotosivashinsky1d</code>","text":""},{"location":"reference/datapipes/#pdearena.data.oned.datapipes.kuramotosivashinsky1d.KuramotoSivashinskyDatasetOpener","title":"<code>KuramotoSivashinskyDatasetOpener</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>DataPipe to load the Kuramoto-Sivashinsky dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>IterDataPipe</code> <p>List of <code>hdf5</code> files containing Kolmogorov data.</p> required <code>mode</code> <code>str</code> <p>Mode to load data from. Can be one of <code>train</code>, <code>val</code>, <code>test</code>.</p> required <code>preload</code> <code>bool</code> <p>Whether to preload all data into memory. Defaults to True.</p> <code>True</code> <code>allow_shuffle</code> <code>bool</code> <p>Whether to shuffle the data, recommended when preloading data. Defaults to True.</p> <code>True</code> <code>resolution</code> <code>int</code> <p>Which resolution to load. Defaults to full data resolution.</p> <code>-1</code> <code>usegrid</code> <code>bool</code> <p>Whether to output spatial grid or not. Defaults to False.</p> <code>False</code> <p>Yields:</p> Type Description <code>Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]</code> <p>Tuple containing particle scalar field, velocity vector field, and optionally buoyancy force parameter value  and spatial grid.</p> Source code in <code>pdearena/data/oned/datapipes/kuramotosivashinsky1d.py</code> <pre><code>class KuramotoSivashinskyDatasetOpener(dp.iter.IterDataPipe):\n    \"\"\"DataPipe to load the Kuramoto-Sivashinsky dataset.\n\n    Args:\n        dp (dp.iter.IterDataPipe): List of `hdf5` files containing Kolmogorov data.\n        mode (str): Mode to load data from. Can be one of `train`, `val`, `test`.\n        preload (bool, optional): Whether to preload all data into memory. Defaults to True.\n        allow_shuffle (bool, optional): Whether to shuffle the data, recommended when preloading data. Defaults to True.\n        resolution (int, optional): Which resolution to load. Defaults to full data resolution.\n        usegrid (bool, optional): Whether to output spatial grid or not. Defaults to False.\n\n    Yields:\n        (Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]): Tuple containing particle scalar field, velocity vector field, and optionally buoyancy force parameter value  and spatial grid.\n    \"\"\"\n\n    def __init__(\n        self,\n        dp,\n        mode: str,\n        preload: bool = True,\n        allow_shuffle: bool = True,\n        resolution: int = -1,\n        usegrid: bool = False,\n        time_step: int = 4,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__()\n        self.dp = dp\n        self.mode = mode\n        self.allow_shuffle = allow_shuffle\n        self.dtype = np.float32\n        self.resolution = resolution\n        self.usegrid = usegrid\n        self.time_step = time_step\n        print(f\"Loading {mode} data from {len([p for p in dp])} files.\")\n        self.storage = {}\n        if preload:\n            for path in self.dp:\n                self.storage[path] = self._load_data(path)\n\n    def _load_data(self, path):\n        if path in self.storage:\n            return self.storage[path]\n        else:\n            with h5py.File(path, \"r\") as f:\n                data_h5 = f[self.mode]\n                data_key = [k for k in data_h5.keys() if k.startswith(\"pde_\")][0]\n                data = {\n                    \"u\": torch.tensor(data_h5[data_key][:].astype(self.dtype)),\n                    \"dt\": torch.tensor(data_h5[\"dt\"][:].astype(self.dtype)),\n                    \"dx\": torch.tensor(data_h5[\"dx\"][:].astype(self.dtype)),\n                }\n                if \"v\" in data_h5:\n                    data[\"v\"] = torch.tensor(data_h5[\"v\"][:].astype(self.dtype))\n\n                data[\"orig_dt\"] = data[\"dt\"].clone()\n                if data[\"u\"].ndim == 3:\n                    data[\"u\"] = data[\"u\"].unsqueeze(dim=-2)  # Add channel dimension\n                # The KS equation is parameterized by [1] the time step between observations\n                # (measured in seconds, usually around 0.2), [2] the spatial step between\n                # data points in the spatial domain (measured in meters, usually around 0.2),\n                # and finally [3] the viscosity parameter (measured in m^2/s, usually between 0.5 - 1.5).\n                # We scale these parameters to be in the range [0, 10] to be visible changes in fourier embeds.\n                # This accelerates learning and makes it easier for the models to learn the conditional dynamics.\n                # Scaling time step.\n                if data[\"dt\"].min() &gt; 0.15 and data[\"dt\"].max() &lt; 0.25:\n                    data[\"dt\"] = (data[\"dt\"] - 0.15) * 100.0\n                else:\n                    print(\n                        f\"WARNING: dt is not in the expected range (min {data['dt'].min()}, max {data['dt'].max()}, mean {data['dt'].mean()}) - scaling may be incorrect.\"\n                    )\n                # Scaling spatial step.\n                if data[\"dx\"].min() &gt; 0.2 and data[\"dx\"].max() &lt; 0.3:\n                    data[\"dx\"] = (data[\"dx\"] - 0.2) * 100.0\n                else:\n                    print(\n                        f\"WARNING: dx is not in the expected range (min {data['dx'].min()}, max {data['dx'].max()}, mean {data['dx'].mean()}) - scaling may be incorrect.\"\n                    )\n                # Scaling viscosity.\n                if \"v\" in data:\n                    if data[\"v\"].min() &gt;= 0.5 and data[\"v\"].max() &lt;= 1.5:\n                        data[\"v\"] = (data[\"v\"] - 0.5) * 100.0\n                    else:\n                        print(\n                            f\"WARNING: v is not in the expected range (min {data['v'].min()}, max {data['v'].max()}, mean {data['v'].mean()}) - scaling may be incorrect.\"\n                        )\n\n            return data\n\n    def __iter__(self):\n        for path in self.dp:\n            data = self._load_data(path)\n            u = data[\"u\"]\n            if u.ndim == 3:\n                u = u.unsqueeze(0)\n            if self.resolution &gt; 0 and u.shape[-1] &gt; self.resolution:\n                step_size = u.shape[-1] // self.resolution\n                start_idx = 0 if not self.mode == \"train\" else np.random.randint(0, step_size)\n                u = u[..., start_idx::step_size]\n            idxs = np.arange(u.shape[0])\n            if self.mode == \"train\" and self.allow_shuffle:\n                np.random.shuffle(idxs)\n            for i in range(idxs.shape[0]):\n                idx = idxs[i]\n                cond = [data[\"dt\"][idx], data[\"dx\"][idx]]\n                if \"v\" in data:\n                    cond.append(data[\"v\"][idx])\n                if self.usegrid:\n                    grid = np.linspace(0, 1, u.shape[-1])\n                else:\n                    grid = None\n                u_sample = u[idx]\n                if self.time_step &gt; 1:\n                    if self.mode == \"train\":\n                        start_idx = np.random.randint(0, self.time_step)\n                    else:\n                        start_idx = 0\n                    u_sample = u_sample[start_idx :: self.time_step]\n                yield u_sample, torch.zeros_like(u_sample[:, 0:0]), torch.tensor(cond)[None], grid\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.twod.datapipes.navierstokes2d","title":"<code>pdearena.data.twod.datapipes.navierstokes2d</code>","text":""},{"location":"reference/datapipes/#pdearena.data.twod.datapipes.navierstokes2d.NavierStokesDatasetOpener","title":"<code>NavierStokesDatasetOpener</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>DataPipe to load Navier-Stokes dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>IterDataPipe</code> <p>List of <code>hdf5</code> files containing Navier-Stokes data.</p> required <code>mode</code> <code>str</code> <p>Mode to load data from. Can be one of <code>train</code>, <code>val</code>, <code>test</code>.</p> required <code>limit_trajectories</code> <code>int</code> <p>Limit the number of trajectories to load from individual <code>hdf5</code> file. Defaults to None.</p> <code>None</code> <code>usegrid</code> <code>bool</code> <p>Whether to output spatial grid or not. Defaults to False.</p> <code>False</code> <p>Yields:</p> Type Description <code>Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]</code> <p>Tuple containing particle scalar field, velocity vector field, and optionally buoyancy force parameter value  and spatial grid.</p> Source code in <code>pdearena/data/twod/datapipes/navierstokes2d.py</code> <pre><code>class NavierStokesDatasetOpener(dp.iter.IterDataPipe):\n    \"\"\"DataPipe to load Navier-Stokes dataset.\n\n    Args:\n        dp (dp.iter.IterDataPipe): List of `hdf5` files containing Navier-Stokes data.\n        mode (str): Mode to load data from. Can be one of `train`, `val`, `test`.\n        limit_trajectories (int, optional): Limit the number of trajectories to load from individual `hdf5` file. Defaults to None.\n        usegrid (bool, optional): Whether to output spatial grid or not. Defaults to False.\n\n    Yields:\n        (Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]): Tuple containing particle scalar field, velocity vector field, and optionally buoyancy force parameter value  and spatial grid.\n    \"\"\"\n\n    def __init__(self, dp, mode: str, limit_trajectories: Optional[int] = None, usegrid: bool = False, conditioned: bool = False) -&gt; None:\n        super().__init__()\n        self.dp = dp\n        self.mode = mode\n        self.limit_trajectories = limit_trajectories\n        self.usegrid = usegrid\n        self.conditioned = conditioned\n\n    def __iter__(self):\n        for path in self.dp:\n            with h5py.File(path, \"r\") as f:\n                data = f[self.mode]\n                if self.limit_trajectories is None or self.limit_trajectories == -1:\n                    num = data[\"u\"].shape[0]\n                else:\n                    num = self.limit_trajectories\n\n                iter_start = 0\n                iter_end = num\n\n                for idx in range(iter_start, iter_end):\n                    u = torch.tensor(data[\"u\"][idx])\n                    vx = torch.tensor(data[\"vx\"][idx])\n                    vy = torch.tensor(data[\"vy\"][idx])\n                    if \"buo_y\" in data and self.conditioned:\n                        cond = torch.tensor(data[\"buo_y\"][idx]).unsqueeze(0).float()\n                    else:\n                        cond = None\n\n                    v = torch.cat((vx[:, None], vy[:, None]), dim=1)\n\n                    if self.usegrid:\n                        gridx = torch.linspace(0, 1, data[\"x\"][idx].shape[0])\n                        gridy = torch.linspace(0, 1, data[\"y\"][idx].shape[0])\n                        gridx = gridx.reshape(\n                            1,\n                            gridx.size(0),\n                            1,\n                        ).repeat(\n                            1,\n                            1,\n                            gridy.size(0),\n                        )\n                        gridy = gridy.reshape(\n                            1,\n                            1,\n                            gridy.size(0),\n                        ).repeat(\n                            1,\n                            gridx.size(1),\n                            1,\n                        )\n                        grid = torch.cat((gridx[:, None], gridy[:, None]), dim=1)\n                    else:\n                        grid = None\n                    yield u.unsqueeze(1).float(), v.float(), cond, grid\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.twod.datapipes.shallowwater2d","title":"<code>pdearena.data.twod.datapipes.shallowwater2d</code>","text":""},{"location":"reference/datapipes/#pdearena.data.twod.datapipes.shallowwater2d.ShallowWaterDatasetOpener","title":"<code>ShallowWaterDatasetOpener</code>","text":"<p>             Bases: <code>IterDataPipe</code></p> <p>DataPipe for loading the shallow water dataset</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>IterDataPipe</code> <p>datapipe with paths to load the dataset from.</p> required <code>mode</code> <code>str</code> <p>\"train\" or \"valid\" or \"test\"</p> required <code>limit_trajectories</code> <code>Optional[int]</code> <p>number of trajectories to load from the dataset</p> <code>None</code> <code>usevort</code> <code>bool</code> <p>whether to use vorticity or velocity. If False, velocity is returned.</p> <code>False</code> <code>usegrid</code> <code>bool</code> <p>whether to use grid or not. If False, no grid is returned.</p> <code>False</code> <code>sample_rate</code> <code>int</code> <p>sample rate for the data. Default is 1, which means no sub-sampling.</p> <code>1</code> Note <p>We manually manage the data distribution across workers and processes. So make sure not to use <code>torchdata</code>'s dp.iter.Sharder with this data pipe.</p> Source code in <code>pdearena/data/twod/datapipes/shallowwater2d.py</code> <pre><code>class ShallowWaterDatasetOpener(dp.iter.IterDataPipe):\n    \"\"\"DataPipe for loading the shallow water dataset\n\n    Args:\n        dp: datapipe with paths to load the dataset from.\n        mode (str): \"train\" or \"valid\" or \"test\"\n        limit_trajectories: number of trajectories to load from the dataset\n        usevort (bool): whether to use vorticity or velocity. If False, velocity is returned.\n        usegrid (bool): whether to use grid or not. If False, no grid is returned.\n        sample_rate: sample rate for the data. Default is 1, which means no sub-sampling.\n\n    Note:\n        We manually manage the data distribution across workers and processes. So make sure not to use `torchdata`'s [dp.iter.Sharder][torchdata.datapipes.iter.ShardingFilter] with this data pipe.\n    \"\"\"\n\n    def __init__(\n        self,\n        dp: dp.iter.IterDataPipe,\n        mode: str,\n        limit_trajectories: Optional[int] = None,\n        usevort: bool = False,\n        usegrid: bool = False,\n        sample_rate: int = 1,\n    ) -&gt; None:\n        super().__init__()\n        self.dp = dp\n        self.mode = mode\n        self.limit_trajectories = limit_trajectories\n        self.usevort = usevort\n        self.usegrid = usegrid\n        self.sample_rate = sample_rate\n\n    def __iter__(self):\n        for path in self.dp:\n            if \"zarr\" in path:\n                data = xr.open_zarr(path)\n            else:\n                # Note that this is much slower\n                data = xr.open_mfdataset(\n                    os.path.join(path, \"seed=*\", \"run*\", \"output.nc\"),\n                    concat_dim=\"b\",\n                    combine=\"nested\",\n                    parallel=True,\n                )\n\n            normstat = torch.load(os.path.join(path, \"..\", \"normstats.pt\"))\n            if self.limit_trajectories is None or self.limit_trajectories == -1:\n                num = data[\"u\"].shape[0]\n            else:\n                num = self.limit_trajectories\n\n            if dist.is_initialized():\n                rank = dist.get_rank()\n                world_size = dist.get_world_size()\n            else:\n                rank = 0\n                world_size = 1\n\n            # Different workers should be using different trajectory batches\n            worker_info = torch.utils.data.get_worker_info()\n            if worker_info is not None:\n                num_workers_per_dist = min(worker_info.num_workers, num)\n                num_shards = num_workers_per_dist * world_size\n                per_worker = int(math.floor(num / float(num_shards)))\n                wid = rank * num_workers_per_dist + worker_info.id\n                iter_start = wid * per_worker\n                iter_end = iter_start + per_worker\n            else:\n                per_dist = int(math.floor(num / float(world_size)))\n                iter_start = rank * per_dist\n                iter_end = iter_start + per_dist\n\n            for idx in range(iter_start, iter_end):\n                if self.usevort:\n                    vort = torch.tensor(data[\"vor\"][idx].to_numpy())\n                    vort = (vort - normstat[\"vor\"][\"mean\"]) / normstat[\"vor\"][\"std\"]\n                else:\n                    u = torch.tensor(data[\"u\"][idx].to_numpy())\n                    v = torch.tensor(data[\"v\"][idx].to_numpy())\n                    vecf = torch.cat((u, v), dim=1)\n\n                pres = torch.tensor(data[\"pres\"][idx].to_numpy())\n\n                pres = (pres - normstat[\"pres\"][\"mean\"]) / normstat[\"pres\"][\"std\"]\n                pres = pres.unsqueeze(1)\n\n                if self.sample_rate &gt; 1:\n                    # TODO: hardocded skip_nt=4\n                    pres = pres[4 :: self.sample_rate]\n                    if self.usevort:\n                        vort = vort[4 :: self.sample_rate]\n                    else:\n                        vecf = vecf[4 :: self.sample_rate]\n                if self.usegrid:\n                    raise NotImplementedError(\"Grid not implemented for weather data\")\n                else:\n                    if self.usevort:\n                        yield torch.cat((pres, vort), dim=1).float(), None, None, None\n                    else:\n                        yield pres.float(), vecf.float(), None, None\n</code></pre>"},{"location":"reference/datapipes/#pdearena.data.threed.datapipes","title":"<code>pdearena.data.threed.datapipes</code>","text":""},{"location":"reference/datapipes/#pdearena.data.threed.datapipes.build_maxwell_datapipes","title":"<code>build_maxwell_datapipes(pde, data_path, limit_trajectories, usegrid, dataset_opener, lister, sharder, filter_fn, mode, time_history=1, time_future=1, time_gap=0, onestep=False)</code>","text":"<p>Build datapipes for training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>pde</code> <code>PDEDataConfig</code> <p>PDE configuration.</p> required <code>data_path</code> <code>str</code> <p>Path to the data.</p> required <code>limit_trajectories</code> <code>int</code> <p>Number of trajectories to use.</p> required <code>usegrid</code> <code>bool</code> <p>Whether to use spatial grid as input.</p> required <code>dataset_opener</code> <code>Callable[..., IterDataPipe]</code> <p>Dataset opener.</p> required <code>lister</code> <code>Callable[..., IterDataPipe]</code> <p>List files.</p> required <code>sharder</code> <code>Callable[..., IterDataPipe]</code> <p>Shard files.</p> required <code>filter_fn</code> <code>Callable[..., IterDataPipe]</code> <p>Filter files.</p> required <code>mode</code> <code>str</code> <p>Mode of the data. [\"train\", \"valid\", \"test\"]</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the past. Defaults to 1.</p> <code>1</code> <code>time_future</code> <code>int</code> <p>Number of time steps in the future. Defaults to 1.</p> <code>1</code> <code>time_gap</code> <code>int</code> <p>Number of time steps between the past and the future to be skipped. Defaults to 0.</p> <code>0</code> <code>onestep</code> <code>bool</code> <p>Whether to use one-step prediction. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dpipe</code> <code>IterDataPipe</code> <p>IterDataPipe for training and evaluation.</p> Source code in <code>pdearena/data/threed/datapipes.py</code> <pre><code>def build_maxwell_datapipes(\n    pde: PDEDataConfig,\n    data_path,\n    limit_trajectories,\n    usegrid: bool,\n    dataset_opener: Callable[..., dp.iter.IterDataPipe],\n    lister: Callable[..., dp.iter.IterDataPipe],\n    sharder: Callable[..., dp.iter.IterDataPipe],\n    filter_fn: Callable[..., dp.iter.IterDataPipe],\n    mode: str,\n    time_history=1,\n    time_future=1,\n    time_gap=0,\n    onestep=False,\n):\n    \"\"\"Build datapipes for training and evaluation.\n\n    Args:\n        pde (PDEDataConfig): PDE configuration.\n        data_path (str): Path to the data.\n        limit_trajectories (int): Number of trajectories to use.\n        usegrid (bool): Whether to use spatial grid as input.\n        dataset_opener (Callable[..., dp.iter.IterDataPipe]): Dataset opener.\n        lister (Callable[..., dp.iter.IterDataPipe]): List files.\n        sharder (Callable[..., dp.iter.IterDataPipe]): Shard files.\n        filter_fn (Callable[..., dp.iter.IterDataPipe]): Filter files.\n        mode (str): Mode of the data. [\"train\", \"valid\", \"test\"]\n        time_history (int, optional): Number of time steps in the past. Defaults to 1.\n        time_future (int, optional): Number of time steps in the future. Defaults to 1.\n        time_gap (int, optional): Number of time steps between the past and the future to be skipped. Defaults to 0.\n        onestep (bool, optional): Whether to use one-step prediction. Defaults to False.\n\n    Returns:\n        dpipe (IterDataPipe): IterDataPipe for training and evaluation.\n    \"\"\"\n    dpipe = lister(\n        data_path,\n    ).filter(filter_fn=filter_fn)\n    if mode == \"train\":\n        dpipe = dpipe.shuffle()\n\n    dpipe = dataset_opener(\n        sharder(dpipe),\n        mode=mode,\n        limit_trajectories=limit_trajectories,\n        usegrid=usegrid,\n    )\n    if mode == \"train\":\n        # Make sure that in expectation we have seen all the data despite randomization\n        dpipe = dpipe.cycle(pde.trajlen)\n\n    if mode == \"train\":\n        # Training data is randomized\n        dpipe = RandomizedPDETrainData3D(\n            dpipe,\n            pde,\n            time_history,\n            time_future,\n            time_gap,\n        )\n    else:\n        # Evaluation data is not randomized.\n        if onestep:\n            dpipe = PDEEvalTimeStepData3D(\n                dpipe,\n                pde,\n                time_history,\n                time_future,\n                time_gap,\n            )\n\n    return dpipe\n</code></pre>"},{"location":"reference/metrics/","title":"Training and Evaluation metrics","text":""},{"location":"reference/metrics/#pdearena.modules.loss.CustomMSELoss","title":"<code>CustomMSELoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Custom MSE loss for PDEs.</p> <p>MSE but summed over time and fields, then averaged over space and batch.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> Source code in <code>pdearena/modules/loss.py</code> <pre><code>class CustomMSELoss(torch.nn.Module):\n    \"\"\"Custom MSE loss for PDEs.\n\n    MSE but summed over time and fields, then averaged over space and batch.\n\n    Args:\n        reduction (str, optional): Reduction method. Defaults to \"mean\".\n    \"\"\"\n\n    def __init__(self, reduction: str = \"mean\") -&gt; None:\n        super().__init__()\n        self.reduction = reduction\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        return custommse_loss(input, target, reduction=self.reduction)\n</code></pre>"},{"location":"reference/metrics/#pdearena.modules.loss.PearsonCorrelationScore","title":"<code>PearsonCorrelationScore</code>","text":"<p>             Bases: <code>Module</code></p> <p>Pearson Correlation Score for PDEs.</p> Source code in <code>pdearena/modules/loss.py</code> <pre><code>class PearsonCorrelationScore(torch.nn.Module):\n    \"\"\"Pearson Correlation Score for PDEs.\"\"\"\n\n    def __init__(self, channel: int = None, reduce_batch: bool = False) -&gt; None:\n        super().__init__()\n        self.channel = channel\n        self.reduce_batch = reduce_batch\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        if self.channel is not None:\n            input = input[:, :, self.channel]\n            target = target[:, :, self.channel]\n        return pearson_correlation(input, target, reduce_batch=self.reduce_batch)\n</code></pre>"},{"location":"reference/metrics/#pdearena.modules.loss.ScaledLpLoss","title":"<code>ScaledLpLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Scaled Lp loss for PDEs.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>p in Lp norm. Defaults to 2.</p> <code>2</code> <code>reduction</code> <code>str</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> Source code in <code>pdearena/modules/loss.py</code> <pre><code>class ScaledLpLoss(torch.nn.Module):\n    \"\"\"Scaled Lp loss for PDEs.\n\n    Args:\n        p (int, optional): p in Lp norm. Defaults to 2.\n        reduction (str, optional): Reduction method. Defaults to \"mean\".\n    \"\"\"\n\n    def __init__(self, p: int = 2, reduction: str = \"mean\") -&gt; None:\n        super().__init__()\n        self.p = p\n        self.reduction = reduction\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        return scaledlp_loss(input, target, p=self.p, reduction=self.reduction)\n</code></pre>"},{"location":"reference/modules/","title":"Available PDE Surrogate Modules","text":""},{"location":"reference/modules/#pdearena.modules.fourier.SpectralConv1d","title":"<code>SpectralConv1d</code>","text":"<p>             Bases: <code>Module</code></p> <p>1D Fourier layer. Does FFT, linear transform, and Inverse FFT. Implemented in a way to allow multi-gpu training. Args:     in_channels (int): Number of input channels     out_channels (int): Number of output channels     modes (int): Number of Fourier modes paper</p> Source code in <code>pdearena/modules/fourier.py</code> <pre><code>class SpectralConv1d(nn.Module):\n    \"\"\"1D Fourier layer. Does FFT, linear transform, and Inverse FFT.\n    Implemented in a way to allow multi-gpu training.\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        modes (int): Number of Fourier modes\n    [paper](https://arxiv.org/abs/2010.08895)\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, modes: int):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes = modes\n\n        self.scale = 1 / (in_channels * out_channels)\n        self.weights = nn.Parameter(\n            self.scale * torch.rand(in_channels, out_channels, self.modes, 2, dtype=torch.float32)\n        )\n\n    def forward(self, x):\n        batchsize = x.shape[0]\n        # Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfft(x)\n\n        # Multiply relevant Fourier modes\n        out_ft = torch.zeros(\n            batchsize,\n            self.out_channels,\n            x.size(-1) // 2 + 1,\n            dtype=torch.cfloat,\n            device=x.device,\n        )\n        out_ft[:, :, : self.modes] = batchmul1d(x_ft[:, :, : self.modes], torch.view_as_complex(self.weights))\n\n        # Return to physical space\n        x = torch.fft.irfft(out_ft, n=x.size(-1))\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.fourier.SpectralConv2d","title":"<code>SpectralConv2d</code>","text":"<p>             Bases: <code>Module</code></p> <p>2D Fourier layer. Does FFT, linear transform, and Inverse FFT. Implemented in a way to allow multi-gpu training. Args:     in_channels (int): Number of input channels     out_channels (int): Number of output channels     modes1 (int): Number of Fourier modes to keep in the first spatial direction     modes2 (int): Number of Fourier modes to keep in the second spatial direction paper</p> Source code in <code>pdearena/modules/fourier.py</code> <pre><code>class SpectralConv2d(nn.Module):\n    \"\"\"2D Fourier layer. Does FFT, linear transform, and Inverse FFT.\n    Implemented in a way to allow multi-gpu training.\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        modes1 (int): Number of Fourier modes to keep in the first spatial direction\n        modes2 (int): Number of Fourier modes to keep in the second spatial direction\n    [paper](https://arxiv.org/abs/2010.08895)\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, modes1: int, modes2: int):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes1 = modes1  # Number of Fourier modes to multiply, at most floor(N/2) + 1\n        self.modes2 = modes2\n\n        self.scale = 1 / (in_channels * out_channels)\n        self.weights1 = nn.Parameter(\n            self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, 2, dtype=torch.float32)\n        )\n        self.weights2 = nn.Parameter(\n            self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, 2, dtype=torch.float32)\n        )\n\n    def forward(self, x, x_dim=None, y_dim=None):\n        batchsize = x.shape[0]\n        # Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfft2(x)\n\n        # Multiply relevant Fourier modes\n        out_ft = torch.zeros(\n            batchsize,\n            self.out_channels,\n            x.size(-2),\n            x.size(-1) // 2 + 1,\n            dtype=torch.cfloat,\n            device=x.device,\n        )\n        out_ft[:, :, : self.modes1, : self.modes2] = batchmul2d(\n            x_ft[:, :, : self.modes1, : self.modes2], torch.view_as_complex(self.weights1)\n        )\n        out_ft[:, :, -self.modes1 :, : self.modes2] = batchmul2d(\n            x_ft[:, :, -self.modes1 :, : self.modes2], torch.view_as_complex(self.weights2)\n        )\n\n        # Return to physical space\n        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.fourier.SpectralConv3d","title":"<code>SpectralConv3d</code>","text":"<p>             Bases: <code>Module</code></p> <p>3D Fourier layer. Does FFT, linear transform, and Inverse FFT. Implemented in a way to allow multi-gpu training. Args:     in_channels (int): Number of input channels     out_channels (int): Number of output channels     modes1 (int): Number of Fourier modes to keep in the first spatial direction     modes2 (int): Number of Fourier modes to keep in the second spatial direction     modes3 (int): Number of Fourier modes to keep in the third spatial direction paper</p> Source code in <code>pdearena/modules/fourier.py</code> <pre><code>class SpectralConv3d(nn.Module):\n    \"\"\"3D Fourier layer. Does FFT, linear transform, and Inverse FFT.\n    Implemented in a way to allow multi-gpu training.\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        modes1 (int): Number of Fourier modes to keep in the first spatial direction\n        modes2 (int): Number of Fourier modes to keep in the second spatial direction\n        modes3 (int): Number of Fourier modes to keep in the third spatial direction\n    [paper](https://arxiv.org/abs/2010.08895)\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, modes1: int, modes2: int, modes3: int):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes1 = modes1  # Number of Fourier modes to multiply, at most floor(N/2) + 1\n        self.modes2 = modes2\n        self.modes3 = modes3\n\n        self.scale = 1 / (in_channels * out_channels)\n        self.weights1 = nn.Parameter(\n            self.scale\n            * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, 2, dtype=torch.float32)\n        )\n        self.weights2 = nn.Parameter(\n            self.scale\n            * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, 2, dtype=torch.float32)\n        )\n        self.weights3 = nn.Parameter(\n            self.scale\n            * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, 2, dtype=torch.float32)\n        )\n        self.weights4 = nn.Parameter(\n            self.scale\n            * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, 2, dtype=torch.float32)\n        )\n\n    def forward(self, x):\n        batchsize = x.shape[0]\n        # Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfftn(x, dim=[-3, -2, -1])\n\n        # Multiply relevant Fourier modes\n        out_ft = torch.zeros(\n            batchsize,\n            self.out_channels,\n            x.size(-3),\n            x.size(-2),\n            x.size(-1) // 2 + 1,\n            dtype=torch.cfloat,\n            device=x.device,\n        )\n        out_ft[:, :, : self.modes1, : self.modes2, : self.modes3] = batchmul3d(\n            x_ft[:, :, : self.modes1, : self.modes2, : self.modes3], torch.view_as_complex(self.weights1)\n        )\n        out_ft[:, :, -self.modes1 :, : self.modes2, : self.modes3] = batchmul3d(\n            x_ft[:, :, -self.modes1 :, : self.modes2, : self.modes3], torch.view_as_complex(self.weights2)\n        )\n        out_ft[:, :, : self.modes1, -self.modes2 :, : self.modes3] = batchmul3d(\n            x_ft[:, :, : self.modes1, -self.modes2 :, : self.modes3], torch.view_as_complex(self.weights3)\n        )\n        out_ft[:, :, -self.modes1 :, -self.modes2 :, : self.modes3] = batchmul3d(\n            x_ft[:, :, -self.modes1 :, -self.modes2 :, : self.modes3], torch.view_as_complex(self.weights4)\n        )\n\n        # Return to physical space\n        x = torch.fft.irfftn(out_ft, s=(x.size(-3), x.size(-2), x.size(-1)))\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_resnet.DilatedBasicBlock","title":"<code>DilatedBasicBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Basic block for Dilated ResNet</p> <p>Parameters:</p> Name Type Description Default <code>in_planes</code> <code>int</code> <p>number of input channels</p> required <code>planes</code> <code>int</code> <p>number of output channels</p> required <code>stride</code> <code>int</code> <p>stride of the convolution. Defaults to 1.</p> <code>1</code> <code>activation</code> <code>str</code> <p>activation function. Defaults to \"relu\".</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>whether to use group normalization. Defaults to True.</p> <code>True</code> <code>num_groups</code> <code>int</code> <p>number of groups for group normalization. Defaults to 1.</p> <code>1</code> Source code in <code>pdearena/modules/twod_resnet.py</code> <pre><code>class DilatedBasicBlock(nn.Module):\n    \"\"\"Basic block for Dilated ResNet\n\n    Args:\n        in_planes (int): number of input channels\n        planes (int): number of output channels\n        stride (int, optional): stride of the convolution. Defaults to 1.\n        activation (str, optional): activation function. Defaults to \"relu\".\n        norm (bool, optional): whether to use group normalization. Defaults to True.\n        num_groups (int, optional): number of groups for group normalization. Defaults to 1.\n    \"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self,\n        in_planes: int,\n        planes: int,\n        stride: int = 1,\n        activation: str = \"relu\",\n        norm: bool = True,\n        num_groups: int = 1,\n    ):\n        super().__init__()\n\n        self.dilation = [1, 2, 4, 8, 4, 2, 1]\n        dilation_layers = []\n        for dil in self.dilation:\n            dilation_layers.append(\n                nn.Conv2d(\n                    in_planes,\n                    planes,\n                    kernel_size=3,\n                    stride=stride,\n                    dilation=dil,\n                    padding=dil,\n                    bias=True,\n                )\n            )\n        self.dilation_layers = nn.ModuleList(dilation_layers)\n        self.norm_layers = nn.ModuleList(\n            nn.GroupNorm(num_groups, num_channels=planes) if norm else nn.Identity() for dil in self.dilation\n        )\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        out = x\n        for layer, norm in zip(self.dilation_layers, self.norm_layers):\n            out = self.activation(layer(norm(out)))\n        return out + x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_resnet.FourierBasicBlock","title":"<code>FourierBasicBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Basic block for Fourier Neural Operators</p> <p>Parameters:</p> Name Type Description Default <code>in_planes</code> <code>int</code> <p>number of input channels</p> required <code>planes</code> <code>int</code> <p>number of output channels</p> required <code>stride</code> <code>int</code> <p>stride of the convolution. Defaults to 1.</p> <code>1</code> <code>modes1</code> <code>int</code> <p>number of modes for the first spatial dimension. Defaults to 16.</p> <code>16</code> <code>modes2</code> <code>int</code> <p>number of modes for the second spatial dimension. Defaults to 16.</p> <code>16</code> <code>activation</code> <code>str</code> <p>activation function. Defaults to \"relu\".</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>whether to use group normalization. Defaults to False.</p> <code>False</code> Source code in <code>pdearena/modules/twod_resnet.py</code> <pre><code>class FourierBasicBlock(nn.Module):\n    \"\"\"Basic block for Fourier Neural Operators\n\n    Args:\n        in_planes (int): number of input channels\n        planes (int): number of output channels\n        stride (int, optional): stride of the convolution. Defaults to 1.\n        modes1 (int, optional): number of modes for the first spatial dimension. Defaults to 16.\n        modes2 (int, optional): number of modes for the second spatial dimension. Defaults to 16.\n        activation (str, optional): activation function. Defaults to \"relu\".\n        norm (bool, optional): whether to use group normalization. Defaults to False.\n\n    \"\"\"\n\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_planes: int,\n        planes: int,\n        stride: int = 1,\n        modes1: int = 16,\n        modes2: int = 16,\n        activation: str = \"gelu\",\n        norm: bool = False,\n    ):\n        super().__init__()\n        self.modes1 = modes1\n        self.modes2 = modes2\n        assert not norm\n        self.fourier1 = SpectralConv2d(in_planes, planes, modes1=self.modes1, modes2=self.modes2)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, padding=0, padding_mode=\"zeros\", bias=True)\n        self.fourier2 = SpectralConv2d(planes, planes, modes1=self.modes1, modes2=self.modes2)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=1, padding=0, padding_mode=\"zeros\", bias=True)\n\n        # So far shortcut connections are not helping\n        # self.shortcut = nn.Sequential()\n        # if stride != 1 or in_planes != self.expansion * planes:\n        #     self.shortcut = nn.Sequential(\n        #         nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1)\n        #     )\n\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x1 = self.fourier1(x)\n        x2 = self.conv1(x)\n        out = self.activation(x1 + x2)\n\n        x1 = self.fourier2(out)\n        x2 = self.conv2(out)\n        out = x1 + x2\n        # out += self.shortcut(x)\n        out = self.activation(out)\n        return out\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_resnet.ResNet","title":"<code>ResNet</code>","text":"<p>             Bases: <code>Module</code></p> <p>Class to support ResNet like feedforward architectures</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of input scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of input vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>block</code> <code>Callable</code> <p>BasicBlock or DilatedBasicBlock or FourierBasicBlock</p> required <code>num_blocks</code> <code>List[int]</code> <p>Number of blocks in each layer</p> required <code>time_history</code> <code>int</code> <p>Number of time steps to use in the input</p> required <code>time_future</code> <code>int</code> <p>Number of time steps to predict in the output</p> required <code>hidden_channels</code> <code>int</code> <p>Number of channels in the hidden layers</p> <code>64</code> <code>activation</code> <code>str</code> <p>Activation function to use</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>True</code> Source code in <code>pdearena/modules/twod_resnet.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"Class to support ResNet like feedforward architectures\n\n    Args:\n        n_input_scalar_components (int): Number of input scalar components in the model\n        n_input_vector_components (int): Number of input vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        block (Callable): BasicBlock or DilatedBasicBlock or FourierBasicBlock\n        num_blocks (List[int]): Number of blocks in each layer\n        time_history (int): Number of time steps to use in the input\n        time_future (int): Number of time steps to predict in the output\n        hidden_channels (int): Number of channels in the hidden layers\n        activation (str): Activation function to use\n        norm (bool): Whether to use normalization\n    \"\"\"\n\n    padding = 9\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        block,\n        num_blocks: list,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int = 64,\n        activation: str = \"gelu\",\n        norm: bool = True,\n        diffmode: bool = False,\n        usegrid: bool = False,\n    ):\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.diffmode = diffmode\n        self.usegrid = usegrid\n        self.in_planes = hidden_channels\n        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        if self.usegrid:\n            insize += 2\n        self.conv_in1 = nn.Conv2d(\n            insize,\n            self.in_planes,\n            kernel_size=1,\n            bias=True,\n        )\n        self.conv_in2 = nn.Conv2d(\n            self.in_planes,\n            self.in_planes,\n            kernel_size=1,\n            bias=True,\n        )\n        self.conv_out1 = nn.Conv2d(\n            self.in_planes,\n            self.in_planes,\n            kernel_size=1,\n            bias=True,\n        )\n        self.conv_out2 = nn.Conv2d(\n            self.in_planes,\n            time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2),\n            kernel_size=1,\n            bias=True,\n        )\n\n        self.layers = nn.ModuleList(\n            [\n                self._make_layer(\n                    block,\n                    self.in_planes,\n                    num_blocks[i],\n                    stride=1,\n                    activation=activation,\n                    norm=norm,\n                )\n                for i in range(len(num_blocks))\n            ]\n        )\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n    def _make_layer(\n        self,\n        block: Callable,\n        planes: int,\n        num_blocks: int,\n        stride: int,\n        activation: str,\n        norm: bool = True,\n    ) -&gt; nn.Sequential:\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(\n                block(\n                    self.in_planes,\n                    planes,\n                    stride,\n                    activation=activation,\n                    norm=norm,\n                )\n            )\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def __repr__(self):\n        return \"ResNet\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n        # prev = x.float()\n        x = self.activation(self.conv_in1(x.float()))\n        x = self.activation(self.conv_in2(x.float()))\n\n        if self.padding &gt; 0:\n            x = F.pad(x, [0, self.padding, 0, self.padding])\n\n        for layer in self.layers:\n            x = layer(x)\n\n        if self.padding &gt; 0:\n            x = x[..., : -self.padding, : -self.padding]\n\n        x = self.activation(self.conv_out1(x))\n        x = self.conv_out2(x)\n\n        if self.diffmode:\n            raise NotImplementedError(\"diffmode\")\n            # x = x + prev[:, -1:, ...].detach()\n        return x.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_uno.OperatorBlock_2D","title":"<code>OperatorBlock_2D</code>","text":"<p>             Bases: <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>in_codim</code> <code>int</code> <p>Input co-domian dimension</p> required <code>out_codim</code> <code>int</code> <p>output co-domain dimension</p> required <code>dim1</code> <code>int</code> <p>Default output grid size along x (or 1st dimension)</p> required <code>dim2</code> <code>int</code> <p>Default output grid size along y ( or 2nd dimension)</p> required <code>modes1</code> <code>int</code> <p>Number of fourier modes to consider along 1st dimension</p> required <code>modes2</code> <code>int</code> <p>Number of fourier modes to consider along 2nd dimension</p> required <code>norm</code> <code>bool</code> <p>Whether to use normalization (torch.nn.InstanceNorm2d)</p> <code>True</code> <code>nonlin</code> <code>bool</code> <p>Whether to use non-linearity (torch.nn.GELU)</p> <code>True</code> <p>All variables are consistent with the <code>SpectralConv2d_Uno</code>.</p> Source code in <code>pdearena/modules/twod_uno.py</code> <pre><code>class OperatorBlock_2D(nn.Module):\n    \"\"\"\n\n    Args:\n        in_codim (int): Input co-domian dimension\n        out_codim (int): output co-domain dimension\n        dim1 (int):  Default output grid size along x (or 1st dimension)\n        dim2 (int): Default output grid size along y ( or 2nd dimension)\n        modes1 (int): Number of fourier modes to consider along 1st dimension\n        modes2 (int): Number of fourier modes to consider along 2nd dimension\n        norm (bool): Whether to use normalization ([torch.nn.InstanceNorm2d][])\n        nonlin (bool): Whether to use non-linearity ([torch.nn.GELU][])\n\n\n    All variables are consistent with the [`SpectralConv2d_Uno`][pdearena.modules.twod_uno.SpectralConv2d_Uno].\n    \"\"\"\n\n    def __init__(self, in_codim, out_codim, dim1, dim2, modes1, modes2, norm=True, nonlin=True):\n        super().__init__()\n        self.conv = SpectralConv2d_Uno(in_codim, out_codim, dim1, dim2, modes1, modes2)\n        self.w = Pointwise_op_2D(in_codim, out_codim, dim1, dim2)\n        self.norm = norm\n        self.non_lin = nonlin\n        if norm:\n            self.normalize_layer = nn.InstanceNorm2d(int(out_codim), affine=True)\n\n    def forward(self, x, dim1=None, dim2=None):\n        #\n        # input shape = (batch, in_codim, input_dim1,input_dim2)\n        # output shape = (batch, out_codim, dim1,dim2)\n        x1_out = self.conv(x, dim1, dim2)\n        x2_out = self.w(x, dim1, dim2)\n        x_out = x1_out + x2_out\n        if self.norm:\n            x_out = self.normalize_layer(x_out)\n        if self.non_lin:\n            x_out = F.gelu(x_out)\n        return x_out\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_uno.Pointwise_op_2D","title":"<code>Pointwise_op_2D</code>","text":"<p>             Bases: <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>in_codim</code> <code>int</code> <p>Input co-domian dimension</p> required <code>out_codim</code> <code>int</code> <p>output co-domain dimension</p> required <code>dim1</code> <code>int</code> <p>Default output grid size along x (or 1st dimension)</p> required <code>dim2</code> <code>int</code> <p>Default output grid size along y ( or 2nd dimension)</p> required Source code in <code>pdearena/modules/twod_uno.py</code> <pre><code>class Pointwise_op_2D(nn.Module):\n    \"\"\"\n\n    Args:\n        in_codim (int): Input co-domian dimension\n        out_codim (int): output co-domain dimension\n\n        dim1 (int):  Default output grid size along x (or 1st dimension)\n        dim2 (int): Default output grid size along y ( or 2nd dimension)\n    \"\"\"\n\n    def __init__(self, in_codim: int, out_codim: int, dim1: int, dim2: int):\n        super().__init__()\n        self.conv = nn.Conv2d(int(in_codim), int(out_codim), 1)\n        self.dim1 = int(dim1)\n        self.dim2 = int(dim2)\n\n    def forward(self, x, dim1=None, dim2=None):\n        #\n        # input shape = (batch, in_codim, input_dim1,input_dim2)\n        # output shape = (batch, out_codim, dim1,dim2)\n\n        if dim1 is None:\n            dim1 = self.dim1\n            dim2 = self.dim2\n        x_out = self.conv(x)\n\n        x_out = F.interpolate(x_out, size=(dim1, dim2), mode=\"bicubic\", align_corners=True, antialias=True)\n        return x_out\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_uno.SpectralConv2d_Uno","title":"<code>SpectralConv2d_Uno</code>","text":"<p>             Bases: <code>Module</code></p> <p>2D Fourier layer. It does FFT, linear transform, and Inverse FFT.</p> <p>Modified to support multi-gpu training.</p> <p>Parameters:</p> Name Type Description Default <code>in_codim</code> <code>int</code> <p>Input co-domian dimension</p> required <code>out_codim</code> <code>int</code> <p>output co-domain dimension</p> required <code>dim1</code> <code>int</code> <p>Default output grid size along x (or 1st dimension of output domain)</p> required <code>dim2</code> <code>int</code> <p>Default output grid size along y ( or 2nd dimension of output domain)         Ratio of grid size of the input and the output implecitely         set the expansion or contraction farctor along each dimension.</p> required <code>modes1</code> <code>int), modes2 (int</code> <p>Number of fourier modes to consider for the ontegral operator         Number of modes must be compatibale with the input grid size         and desired output grid size.         i.e., modes1 &lt;= min( dim1/2, input_dim1/2).         Here \"input_dim1\" is the grid size along x axis (or first dimension) of the input domain.         Other modes also the have same constrain.</p> <code>None</code> Source code in <code>pdearena/modules/twod_uno.py</code> <pre><code>class SpectralConv2d_Uno(nn.Module):\n    \"\"\"2D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n\n    Modified to support multi-gpu training.\n\n    Args:\n        in_codim (int): Input co-domian dimension\n        out_codim (int): output co-domain dimension\n\n        dim1 (int): Default output grid size along x (or 1st dimension of output domain)\n        dim2 (int): Default output grid size along y ( or 2nd dimension of output domain)\n                    Ratio of grid size of the input and the output implecitely\n                    set the expansion or contraction farctor along each dimension.\n        modes1 (int), modes2 (int):  Number of fourier modes to consider for the ontegral operator\n                    Number of modes must be compatibale with the input grid size\n                    and desired output grid size.\n                    i.e., modes1 &lt;= min( dim1/2, input_dim1/2).\n                    Here \"input_dim1\" is the grid size along x axis (or first dimension) of the input domain.\n                    Other modes also the have same constrain.\n    \"\"\"\n\n    def __init__(self, in_codim, out_codim, dim1, dim2, modes1=None, modes2=None):\n        super().__init__()\n\n        in_codim = int(in_codim)\n        out_codim = int(out_codim)\n        self.in_channels = in_codim\n        self.out_channels = out_codim\n        self.dim1 = dim1\n        self.dim2 = dim2\n        if modes1 is not None:\n            self.modes1 = modes1\n            self.modes2 = modes2\n        else:\n            self.modes1 = dim1 // 2 - 1\n            self.modes2 = dim2 // 2\n        self.scale = (1 / (2 * in_codim)) ** (1.0 / 2.0)\n        self.weights1 = nn.Parameter(\n            self.scale * (torch.randn(in_codim, out_codim, self.modes1, self.modes2, 2, dtype=torch.float32))\n        )\n        self.weights2 = nn.Parameter(\n            self.scale * (torch.randn(in_codim, out_codim, self.modes1, self.modes2, 2, dtype=torch.float32))\n        )\n\n    # Complex multiplication\n    def compl_mul2d(self, input, weights):\n        return torch.einsum(\"bixy,ioxy-&gt;boxy\", input, weights)\n\n    def forward(self, x, dim1=None, dim2=None):\n        if dim1 is not None:\n            self.dim1 = dim1\n            self.dim2 = dim2\n        batchsize = x.shape[0]\n        # Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfft2(x, norm=\"forward\")\n\n        # Multiply relevant Fourier modes\n        out_ft = torch.zeros(\n            batchsize,\n            self.out_channels,\n            self.dim1,\n            self.dim2 // 2 + 1,\n            dtype=torch.cfloat,\n            device=x.device,\n        )\n        out_ft[:, :, : self.modes1, : self.modes2] = self.compl_mul2d(\n            x_ft[:, :, : self.modes1, : self.modes2], torch.view_as_complex(self.weights1)\n        )\n        out_ft[:, :, -self.modes1 :, : self.modes2] = self.compl_mul2d(\n            x_ft[:, :, -self.modes1 :, : self.modes2], torch.view_as_complex(self.weights2)\n        )\n\n        # Return to physical space\n        x = torch.fft.irfft2(out_ft, s=(self.dim1, self.dim2), norm=\"forward\")\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_uno.UNO","title":"<code>UNO</code>","text":"<p>             Bases: <code>Module</code></p> <p>UNO model</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps to include in the model</p> required <code>time_future</code> <code>int</code> <p>Number of time steps to predict in the model</p> required <code>hidden_channels</code> <code>int</code> <p>Number of hidden channels in the model</p> required <code>pad</code> <code>int</code> <p>Padding to use in the model</p> <code>0</code> <code>factor</code> <code>int</code> <p>Scaling factor to use in the model</p> <code>3 / 4</code> <code>activation</code> <code>str</code> <p>Activation function to use in the model</p> <code>'gelu'</code> Source code in <code>pdearena/modules/twod_uno.py</code> <pre><code>class UNO(nn.Module):\n    \"\"\"UNO model\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps to include in the model\n        time_future (int): Number of time steps to predict in the model\n        hidden_channels (int): Number of hidden channels in the model\n        pad (int): Padding to use in the model\n        factor (int): Scaling factor to use in the model\n        activation (str): Activation function to use in the model\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        pad=0,\n        factor=3 / 4,\n        activation=\"gelu\",\n    ):\n        super().__init__()\n\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n\n        self.width = hidden_channels\n        self.factor = factor\n        self.padding = pad\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        in_width = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        out_width = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n        self.fc = nn.Linear(in_width, self.width // 2)\n\n        self.fc0 = nn.Linear(self.width // 2, self.width)  # input channel is 3: (a(x, y), x, y)\n\n        self.L0 = OperatorBlock_2D(self.width, 2 * factor * self.width, 48, 48, 18, 18)\n\n        self.L1 = OperatorBlock_2D(2 * factor * self.width, 4 * factor * self.width, 32, 32, 14, 14)\n\n        self.L2 = OperatorBlock_2D(4 * factor * self.width, 8 * factor * self.width, 16, 16, 6, 6)\n\n        self.L3 = OperatorBlock_2D(8 * factor * self.width, 8 * factor * self.width, 16, 16, 6, 6)\n\n        self.L4 = OperatorBlock_2D(8 * factor * self.width, 4 * factor * self.width, 32, 32, 6, 6)\n\n        self.L5 = OperatorBlock_2D(8 * factor * self.width, 2 * factor * self.width, 48, 48, 14, 14)\n\n        self.L6 = OperatorBlock_2D(4 * factor * self.width, self.width, 64, 64, 18, 18)  # will be reshaped\n\n        self.fc1 = nn.Linear(2 * self.width, 4 * self.width)\n        self.fc2 = nn.Linear(4 * self.width, out_width)\n\n    def forward(self, x):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n\n        x = x.permute(0, 2, 3, 1)\n        x_fc = self.fc(x)\n        x_fc = self.activation(x_fc)\n\n        x_fc0 = self.fc0(x_fc)\n        x_fc0 = self.activation(x_fc0)\n\n        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n\n        x_fc0 = F.pad(x_fc0, [self.padding, self.padding, self.padding, self.padding])\n\n        D1, D2 = x_fc0.shape[-2], x_fc0.shape[-1]\n\n        x_c0 = self.L0(x_fc0, int(D1 * self.factor), int(D2 * self.factor))\n        x_c1 = self.L1(x_c0, D1 // 2, D2 // 2)\n\n        x_c2 = self.L2(x_c1, D1 // 4, D2 // 4)\n        x_c3 = self.L3(x_c2, D1 // 4, D2 // 4)\n        x_c4 = self.L4(x_c3, D1 // 2, D2 // 2)\n        x_c4 = torch.cat([x_c4, x_c1], dim=1)\n        x_c5 = self.L5(x_c4, int(D1 * self.factor), int(D2 * self.factor))\n        x_c5 = torch.cat([x_c5, x_c0], dim=1)\n        x_c6 = self.L6(x_c5, D1, D2)\n        x_c6 = torch.cat([x_c6, x_fc0], dim=1)\n\n        if self.padding != 0:\n            x_c6 = x_c6[..., : -self.padding, : -self.padding]\n\n        x_c6 = x_c6.permute(0, 2, 3, 1)\n\n        x_fc1 = self.fc1(x_c6)\n        x_fc1 = self.activation(x_fc1)\n\n        x_out = self.fc2(x_fc1)\n        x_out = x_out.permute(0, 3, 1, 2)\n\n        return x_out.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet2015.Unet2015","title":"<code>Unet2015</code>","text":"<p>             Bases: <code>Module</code></p> <p>Two-dimensional UNet based on original architecture.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the input.</p> required <code>time_future</code> <code>int</code> <p>Number of time steps in the output.</p> required <code>hidden_channels</code> <code>int</code> <p>Number of hidden channels.</p> required <code>activation</code> <code>str</code> <p>Activation function.</p> required Source code in <code>pdearena/modules/twod_unet2015.py</code> <pre><code>class Unet2015(nn.Module):\n    \"\"\"Two-dimensional UNet based on original architecture.\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps in the input.\n        time_future (int): Number of time steps in the output.\n        hidden_channels (int): Number of hidden channels.\n        activation (str): Activation function.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        activation: str,\n    ) -&gt; None:\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.time_history = time_history\n        self.time_future = time_future\n        self.hidden_channels = hidden_channels\n        self.activation = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        in_channels = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n\n        features = hidden_channels\n        self.encoder1 = Unet2015._block(in_channels, features, name=\"enc1\", activation=self.activation)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = Unet2015._block(features, features * 2, name=\"enc2\", activation=self.activation)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = Unet2015._block(features * 2, features * 4, name=\"enc3\", activation=self.activation)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = Unet2015._block(features * 4, features * 8, name=\"enc4\", activation=self.activation)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = Unet2015._block(features * 8, features * 16, name=\"bottleneck\", activation=self.activation)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = Unet2015._block((features * 8) * 2, features * 8, name=\"dec4\", activation=self.activation)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = Unet2015._block((features * 4) * 2, features * 4, name=\"dec3\", activation=self.activation)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = Unet2015._block((features * 2) * 2, features * 2, name=\"dec2\", activation=self.activation)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = Unet2015._block(features * 2, features, name=\"dec1\", activation=self.activation)\n\n        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n\n    def forward(self, x):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n        out = self.conv(dec1)\n        return out.reshape(orig_shape[0], -1, *orig_shape[2:])\n\n    @staticmethod\n    def _block(in_channels, features, name, activation=nn.Tanh()):\n        return nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        name + \"conv1\",\n                        nn.Conv2d(\n                            in_channels=in_channels,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"act1\", activation),\n                    (\n                        name + \"conv2\",\n                        nn.Conv2d(\n                            in_channels=features,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"act2\", activation),\n                ]\n            )\n        )\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unetbase.Unetbase","title":"<code>Unetbase</code>","text":"<p>             Bases: <code>Module</code></p> <p>Our interpretation of the original U-Net architecture.</p> <p>Uses torch.nn.GroupNorm instead of torch.nn.BatchNorm2d. Also there is no <code>BottleNeck</code> block.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the input.</p> required <code>time_future</code> <code>int</code> <p>Number of time steps in the output.</p> required <code>hidden_channels</code> <code>int</code> <p>Number of channels in the hidden layers.</p> required <code>activation</code> <code>str</code> <p>Activation function to use. One of [\"gelu\", \"relu\", \"silu\"].</p> <code>'gelu'</code> Source code in <code>pdearena/modules/twod_unetbase.py</code> <pre><code>class Unetbase(nn.Module):\n    \"\"\"Our interpretation of the original U-Net architecture.\n\n    Uses [torch.nn.GroupNorm][] instead of [torch.nn.BatchNorm2d][]. Also there is no `BottleNeck` block.\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps in the input.\n        time_future (int): Number of time steps in the output.\n        hidden_channels (int): Number of channels in the hidden layers.\n        activation (str): Activation function to use. One of [\"gelu\", \"relu\", \"silu\"].\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        activation=\"gelu\",\n    ) -&gt; None:\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.time_history = time_history\n        self.time_future = time_future\n        self.hidden_channels = hidden_channels\n        self.activation = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        n_channels = hidden_channels\n        self.image_proj = ConvBlock(insize, n_channels, activation=activation)\n\n        self.down = nn.ModuleList(\n            [\n                Down(n_channels, n_channels * 2, activation=activation),\n                Down(n_channels * 2, n_channels * 4, activation=activation),\n                Down(n_channels * 4, n_channels * 8, activation=activation),\n                Down(n_channels * 8, n_channels * 16, activation=activation),\n            ]\n        )\n        self.up = nn.ModuleList(\n            [\n                Up(n_channels * 16, n_channels * 8, activation=activation),\n                Up(n_channels * 8, n_channels * 4, activation=activation),\n                Up(n_channels * 4, n_channels * 2, activation=activation),\n                Up(n_channels * 2, n_channels, activation=activation),\n            ]\n        )\n        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n        # should there be a final norm too? but we aren't doing \"prenorm\" in the original\n        self.final = nn.Conv2d(n_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n\n    def forward(self, x):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])\n        h = self.image_proj(x)\n\n        x1 = self.down[0](h)\n        x2 = self.down[1](x1)\n        x3 = self.down[2](x2)\n        x4 = self.down[3](x3)\n        x = self.up[0](x4, x3)\n        x = self.up[1](x, x2)\n        x = self.up[2](x, x1)\n        x = self.up[3](x, h)\n\n        x = self.final(x)\n        return x.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.AttentionBlock","title":"<code>AttentionBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Attention block This is similar to [transformer multi-head attention]</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>the number of channels in the input</p> required <code>n_heads</code> <code>int</code> <p>the number of heads in multi-head attention</p> <code>1</code> <code>d_k</code> <code>Optional[int]</code> <p>the number of dimensions in each head</p> <code>None</code> <code>n_groups</code> <code>int</code> <p>the number of groups for group normalization.</p> <code>1</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class AttentionBlock(nn.Module):\n    \"\"\"Attention block This is similar to [transformer multi-head\n    attention]\n\n    Args:\n        n_channels (int): the number of channels in the input\n        n_heads (int): the number of heads in multi-head attention\n        d_k: the number of dimensions in each head\n        n_groups (int): the number of groups for [group normalization][torch.nn.GroupNorm].\n\n    \"\"\"\n\n    def __init__(self, n_channels: int, n_heads: int = 1, d_k: Optional[int] = None, n_groups: int = 1):\n        super().__init__()\n\n        # Default `d_k`\n        if d_k is None:\n            d_k = n_channels\n        # Normalization layer\n        self.norm = nn.GroupNorm(n_groups, n_channels)\n        # Projections for query, key and values\n        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n        # Linear layer for final transformation\n        self.output = nn.Linear(n_heads * d_k, n_channels)\n        # Scale for dot-product attention\n        self.scale = d_k**-0.5\n        #\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n    def forward(self, x: torch.Tensor):\n        # Get shape\n        batch_size, n_channels, height, width = x.shape\n        # Change `x` to shape `[batch_size, seq, n_channels]`\n        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        attn = torch.einsum(\"bihd,bjhd-&gt;bijh\", q, k) * self.scale\n        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = attn.softmax(dim=1)\n        # Multiply by values\n        res = torch.einsum(\"bijh,bjhd-&gt;bihd\", attn, v)\n        # Reshape to `[batch_size, seq, n_heads * d_k]`\n        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n        # Transform to `[batch_size, seq, n_channels]`\n        res = self.output(res)\n\n        # Add skip connection\n        res += x\n\n        # Change to shape `[batch_size, in_channels, height, width]`\n        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n        return res\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.DownBlock","title":"<code>DownBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Down block This combines <code>ResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the first half of U-Net at each resolution.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>has_attn</code> <code>bool</code> <p>Whether to use attention block</p> <code>False</code> <code>activation</code> <code>Module</code> <p>Activation function</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>False</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class DownBlock(nn.Module):\n    \"\"\"Down block This combines [`ResidualBlock`][pdearena.modules.twod_unet.ResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n\n    These are used in the first half of U-Net at each resolution.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        has_attn (bool): Whether to use attention block\n        activation (nn.Module): Activation function\n        norm (bool): Whether to use normalization\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n    ):\n        super().__init__()\n        self.res = ResidualBlock(in_channels, out_channels, activation=activation, norm=norm)\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        x = self.res(x)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.Downsample","title":"<code>Downsample</code>","text":"<p>             Bases: <code>Module</code></p> <p>Scale down the feature map by \\(\\frac{1}{2} \\times\\)</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of channels in the input and output.</p> required Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class Downsample(nn.Module):\n    r\"\"\"Scale down the feature map by $\\frac{1}{2} \\times$\n\n    Args:\n        n_channels (int): Number of channels in the input and output.\n    \"\"\"\n\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor):\n        return self.conv(x)\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.FourierDownBlock","title":"<code>FourierDownBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Down block This combines <code>FourierResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the first half of U-Net at each resolution.</p> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class FourierDownBlock(nn.Module):\n    \"\"\"Down block This combines [`FourierResidualBlock`][pdearena.modules.twod_unet.FourierResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n\n    These are used in the first half of U-Net at each resolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        modes1: int = 16,\n        modes2: int = 16,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n    ):\n        super().__init__()\n        self.res = FourierResidualBlock(\n            in_channels,\n            out_channels,\n            modes1=modes1,\n            modes2=modes2,\n            activation=activation,\n            norm=norm,\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        x = self.res(x)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.FourierResidualBlock","title":"<code>FourierResidualBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Fourier Residual Block to be used in modern Unet architectures.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>modes1</code> <code>int</code> <p>Number of modes in the first dimension.</p> <code>16</code> <code>modes2</code> <code>int</code> <p>Number of modes in the second dimension.</p> <code>16</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization.</p> <code>False</code> <code>n_groups</code> <code>int</code> <p>Number of groups for group normalization.</p> <code>1</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class FourierResidualBlock(nn.Module):\n    \"\"\"Fourier Residual Block to be used in modern Unet architectures.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        modes1 (int): Number of modes in the first dimension.\n        modes2 (int): Number of modes in the second dimension.\n        activation (str): Activation function to use.\n        norm (bool): Whether to use normalization.\n        n_groups (int): Number of groups for group normalization.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        modes1: int = 16,\n        modes2: int = 16,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        n_groups: int = 1,\n    ):\n        super().__init__()\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        self.modes1 = modes1\n        self.modes2 = modes2\n\n        self.fourier1 = SpectralConv2d(in_channels, out_channels, modes1=self.modes1, modes2=self.modes2)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, padding_mode=\"zeros\")\n        self.fourier2 = SpectralConv2d(out_channels, out_channels, modes1=self.modes1, modes2=self.modes2)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, padding_mode=\"zeros\")\n        # If the number of input channels is not equal to the number of output channels we have to\n        # project the shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        else:\n            self.shortcut = nn.Identity()\n\n        if norm:\n            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n        else:\n            self.norm1 = nn.Identity()\n            self.norm2 = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        # using pre-norms\n        h = self.activation(self.norm1(x))\n        x1 = self.fourier1(h)\n        x2 = self.conv1(h)\n        out = x1 + x2\n        out = self.activation(self.norm2(out))\n        x1 = self.fourier2(out)\n        x2 = self.conv2(out)\n        out = x1 + x2 + self.shortcut(x)\n        return out\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.FourierUnet","title":"<code>FourierUnet</code>","text":"<p>             Bases: <code>Module</code></p> <p>Unet with Fourier layers in early downsampling blocks.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the input.</p> required <code>time_future</code> <code>int</code> <p>Number of time steps in the output.</p> required <code>hidden_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> required <code>modes1</code> <code>int</code> <p>Number of Fourier modes to use in the first spatial dimension.</p> <code>12</code> <code>modes2</code> <code>int</code> <p>Number of Fourier modes to use in the second spatial dimension.</p> <code>12</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization.</p> <code>False</code> <code>ch_mults</code> <code>list</code> <p>List of integers to multiply the number of channels by at each resolution.</p> <code>(1, 2, 2, 4)</code> <code>is_attn</code> <code>list</code> <p>List of booleans indicating whether to use attention at each resolution.</p> <code>(False, False, False, False)</code> <code>mid_attn</code> <code>bool</code> <p>Whether to use attention in the middle block.</p> <code>False</code> <code>n_blocks</code> <code>int</code> <p>Number of blocks to use at each resolution.</p> <code>2</code> <code>n_fourier_layers</code> <code>int</code> <p>Number of early downsampling layers to use Fourier layers in.</p> <code>2</code> <code>mode_scaling</code> <code>bool</code> <p>Whether to scale the number of modes with resolution.</p> <code>True</code> <code>use1x1</code> <code>bool</code> <p>Whether to use 1x1 convolutions in the initial and final layer.</p> <code>False</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class FourierUnet(nn.Module):\n    \"\"\"Unet with Fourier layers in early downsampling blocks.\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps in the input.\n        time_future (int): Number of time steps in the output.\n        hidden_channels (int): Number of channels in the first layer.\n        activation (str): Activation function to use.\n        modes1 (int): Number of Fourier modes to use in the first spatial dimension.\n        modes2 (int): Number of Fourier modes to use in the second spatial dimension.\n        norm (bool): Whether to use normalization.\n        ch_mults (list): List of integers to multiply the number of channels by at each resolution.\n        is_attn (list): List of booleans indicating whether to use attention at each resolution.\n        mid_attn (bool): Whether to use attention in the middle block.\n        n_blocks (int): Number of blocks to use at each resolution.\n        n_fourier_layers (int): Number of early downsampling layers to use Fourier layers in.\n        mode_scaling (bool): Whether to scale the number of modes with resolution.\n        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        activation: str,\n        modes1: int = 12,\n        modes2: int = 12,\n        norm: bool = False,\n        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n        mid_attn: bool = False,\n        n_blocks: int = 2,\n        n_fourier_layers: int = 2,\n        mode_scaling: bool = True,\n        use1x1: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.time_history = time_history\n        self.time_future = time_future\n        self.hidden_channels = hidden_channels\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n        # Number of resolutions\n        n_resolutions = len(ch_mults)\n\n        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        n_channels = hidden_channels\n        # Project image into feature map\n        if use1x1:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n        else:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # #### First half of U-Net - decreasing resolution\n        down = []\n        # Number of channels\n        out_channels = in_channels = n_channels\n        # For each resolution\n        for i in range(n_resolutions):\n            # Number of output channels at this resolution\n            out_channels = in_channels * ch_mults[i]\n            if i &lt; n_fourier_layers:\n                for _ in range(n_blocks):\n                    down.append(\n                        FourierDownBlock(\n                            in_channels,\n                            out_channels,\n                            modes1=max(modes1 // 2**i, 4) if mode_scaling else modes1,\n                            modes2=max(modes2 // 2**i, 4) if mode_scaling else modes2,\n                            has_attn=is_attn[i],\n                            activation=activation,\n                            norm=norm,\n                        )\n                    )\n                    in_channels = out_channels\n            else:\n                # Add `n_blocks`\n                for _ in range(n_blocks):\n                    down.append(\n                        DownBlock(\n                            in_channels,\n                            out_channels,\n                            has_attn=is_attn[i],\n                            activation=activation,\n                            norm=norm,\n                        )\n                    )\n                    in_channels = out_channels\n            # Down sample at all resolutions except the last\n            if i &lt; n_resolutions - 1:\n                down.append(Downsample(in_channels))\n\n        # Combine the set of modules\n        self.down = nn.ModuleList(down)\n\n        # Middle block\n        self.middle = MiddleBlock(out_channels, has_attn=mid_attn, activation=activation, norm=norm)\n\n        # #### Second half of U-Net - increasing resolution\n        up = []\n        # Number of channels\n        in_channels = out_channels\n        # For each resolution\n        for i in reversed(range(n_resolutions)):\n            # `n_blocks` at the same resolution\n            out_channels = in_channels\n            for _ in range(n_blocks):\n                up.append(\n                    UpBlock(\n                        in_channels,\n                        out_channels,\n                        has_attn=is_attn[i],\n                        activation=activation,\n                        norm=norm,\n                    )\n                )\n            # Final block to reduce the number of channels\n            out_channels = in_channels // ch_mults[i]\n            up.append(UpBlock(in_channels, out_channels, has_attn=is_attn[i], activation=activation, norm=norm))\n            in_channels = out_channels\n            # Up sample at all resolutions except last\n            if i &gt; 0:\n                up.append(Upsample(in_channels))\n\n        # Combine the set of modules\n        self.up = nn.ModuleList(up)\n\n        if norm:\n            self.norm = nn.GroupNorm(8, n_channels)\n        else:\n            self.norm = nn.Identity()\n        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n        if use1x1:\n            self.final = nn.Conv2d(n_channels, out_channels, kernel_size=1)\n        else:\n            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n\n    def forward(self, x: torch.Tensor):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n        x = self.image_proj(x)\n\n        h = [x]\n        for m in self.down:\n            x = m(x)\n            h.append(x)\n\n        x = self.middle(x)\n\n        for m in self.up:\n            if isinstance(m, Upsample):\n                x = m(x)\n            else:\n                # Get the skip connection from first half of U-Net and concatenate\n                s = h.pop()\n                x = torch.cat((x, s), dim=1)\n                #\n                x = m(x)\n\n        x = self.final(self.activation(self.norm(x)))\n        return x.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.FourierUpBlock","title":"<code>FourierUpBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Up block that combines <code>FourierResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the second half of U-Net at each resolution.</p> Note <p>We currently don't recommend using this block.</p> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class FourierUpBlock(nn.Module):\n    \"\"\"Up block that combines [`FourierResidualBlock`][pdearena.modules.twod_unet.FourierResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n\n    These are used in the second half of U-Net at each resolution.\n\n    Note:\n        We currently don't recommend using this block.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        modes1: int = 16,\n        modes2: int = 16,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n    ):\n        super().__init__()\n        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n        # from the first half of the U-Net\n        self.res = FourierResidualBlock(\n            in_channels + out_channels,\n            out_channels,\n            modes1=modes1,\n            modes2=modes2,\n            activation=activation,\n            norm=norm,\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        x = self.res(x)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.MiddleBlock","title":"<code>MiddleBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Middle block</p> <p>It combines a <code>ResidualBlock</code>, <code>AttentionBlock</code>, followed by another <code>ResidualBlock</code>.</p> <p>This block is applied at the lowest resolution of the U-Net.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of channels in the input and output.</p> required <code>has_attn</code> <code>bool</code> <p>Whether to use attention block. Defaults to False.</p> <code>False</code> <code>activation</code> <code>str</code> <p>Activation function to use. Defaults to \"gelu\".</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization. Defaults to False.</p> <code>False</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class MiddleBlock(nn.Module):\n    \"\"\"Middle block\n\n    It combines a `ResidualBlock`, `AttentionBlock`, followed by another\n    `ResidualBlock`.\n\n    This block is applied at the lowest resolution of the U-Net.\n\n    Args:\n        n_channels (int): Number of channels in the input and output.\n        has_attn (bool, optional): Whether to use attention block. Defaults to False.\n        activation (str): Activation function to use. Defaults to \"gelu\".\n        norm (bool, optional): Whether to use normalization. Defaults to False.\n    \"\"\"\n\n    def __init__(self, n_channels: int, has_attn: bool = False, activation: str = \"gelu\", norm: bool = False):\n        super().__init__()\n        self.res1 = ResidualBlock(n_channels, n_channels, activation=activation, norm=norm)\n        self.attn = AttentionBlock(n_channels) if has_attn else nn.Identity()\n        self.res2 = ResidualBlock(n_channels, n_channels, activation=activation, norm=norm)\n\n    def forward(self, x: torch.Tensor):\n        x = self.res1(x)\n        x = self.attn(x)\n        x = self.res2(x)\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.ResidualBlock","title":"<code>ResidualBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Wide Residual Blocks used in modern Unet architectures.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization.</p> <code>False</code> <code>n_groups</code> <code>int</code> <p>Number of groups for group normalization.</p> <code>1</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class ResidualBlock(nn.Module):\n    \"\"\"Wide Residual Blocks used in modern Unet architectures.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        activation (str): Activation function to use.\n        norm (bool): Whether to use normalization.\n        n_groups (int): Number of groups for group normalization.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        n_groups: int = 1,\n    ):\n        super().__init__()\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n        # If the number of input channels is not equal to the number of output channels we have to\n        # project the shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        else:\n            self.shortcut = nn.Identity()\n\n        if norm:\n            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n        else:\n            self.norm1 = nn.Identity()\n            self.norm2 = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        # First convolution layer\n        h = self.conv1(self.activation(self.norm1(x)))\n        # Second convolution layer\n        h = self.conv2(self.activation(self.norm2(h)))\n        # Add the shortcut connection and return\n        return h + self.shortcut(x)\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.Unet","title":"<code>Unet</code>","text":"<p>             Bases: <code>Module</code></p> <p>Modern U-Net architecture</p> <p>This is a modern U-Net architecture with wide-residual blocks and spatial attention blocks</p> <p>Parameters:</p> Name Type Description Default <code>n_input_scalar_components</code> <code>int</code> <p>Number of scalar components in the model</p> required <code>n_input_vector_components</code> <code>int</code> <p>Number of vector components in the model</p> required <code>n_output_scalar_components</code> <code>int</code> <p>Number of output scalar components in the model</p> required <code>n_output_vector_components</code> <code>int</code> <p>Number of output vector components in the model</p> required <code>time_history</code> <code>int</code> <p>Number of time steps in the input</p> required <code>time_future</code> <code>int</code> <p>Number of time steps in the output</p> required <code>hidden_channels</code> <code>int</code> <p>Number of channels in the hidden layers</p> required <code>activation</code> <code>str</code> <p>Activation function to use</p> required <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>False</code> <code>ch_mults</code> <code>list</code> <p>List of channel multipliers for each resolution</p> <code>(1, 2, 2, 4)</code> <code>is_attn</code> <code>list</code> <p>List of booleans indicating whether to use attention blocks</p> <code>(False, False, False, False)</code> <code>mid_attn</code> <code>bool</code> <p>Whether to use attention block in the middle block</p> <code>False</code> <code>n_blocks</code> <code>int</code> <p>Number of residual blocks in each resolution</p> <code>2</code> <code>use1x1</code> <code>bool</code> <p>Whether to use 1x1 convolutions in the initial and final layers</p> <code>False</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class Unet(nn.Module):\n    \"\"\"Modern U-Net architecture\n\n    This is a modern U-Net architecture with wide-residual blocks and spatial attention blocks\n\n    Args:\n        n_input_scalar_components (int): Number of scalar components in the model\n        n_input_vector_components (int): Number of vector components in the model\n        n_output_scalar_components (int): Number of output scalar components in the model\n        n_output_vector_components (int): Number of output vector components in the model\n        time_history (int): Number of time steps in the input\n        time_future (int): Number of time steps in the output\n        hidden_channels (int): Number of channels in the hidden layers\n        activation (str): Activation function to use\n        norm (bool): Whether to use normalization\n        ch_mults (list): List of channel multipliers for each resolution\n        is_attn (list): List of booleans indicating whether to use attention blocks\n        mid_attn (bool): Whether to use attention block in the middle block\n        n_blocks (int): Number of residual blocks in each resolution\n        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layers\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input_scalar_components: int,\n        n_input_vector_components: int,\n        n_output_scalar_components: int,\n        n_output_vector_components: int,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        activation: str,\n        norm: bool = False,\n        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n        mid_attn: bool = False,\n        n_blocks: int = 2,\n        use1x1: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.n_input_scalar_components = n_input_scalar_components\n        self.n_input_vector_components = n_input_vector_components\n        self.n_output_scalar_components = n_output_scalar_components\n        self.n_output_vector_components = n_output_vector_components\n        self.time_history = time_history\n        self.time_future = time_future\n        self.hidden_channels = hidden_channels\n\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n        # Number of resolutions\n        n_resolutions = len(ch_mults)\n\n        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n        n_channels = hidden_channels\n        # Project image into feature map\n        if use1x1:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n        else:\n            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # #### First half of U-Net - decreasing resolution\n        down = []\n        # Number of channels\n        out_channels = in_channels = n_channels\n        # For each resolution\n        for i in range(n_resolutions):\n            # Number of output channels at this resolution\n            out_channels = in_channels * ch_mults[i]\n            # Add `n_blocks`\n            for _ in range(n_blocks):\n                down.append(\n                    DownBlock(\n                        in_channels,\n                        out_channels,\n                        has_attn=is_attn[i],\n                        activation=activation,\n                        norm=norm,\n                    )\n                )\n                in_channels = out_channels\n            # Down sample at all resolutions except the last\n            if i &lt; n_resolutions - 1:\n                down.append(Downsample(in_channels))\n\n        # Combine the set of modules\n        self.down = nn.ModuleList(down)\n\n        # Middle block\n        self.middle = MiddleBlock(out_channels, has_attn=mid_attn, activation=activation, norm=norm)\n\n        # #### Second half of U-Net - increasing resolution\n        up = []\n        # Number of channels\n        in_channels = out_channels\n        # For each resolution\n        for i in reversed(range(n_resolutions)):\n            # `n_blocks` at the same resolution\n            out_channels = in_channels\n            for _ in range(n_blocks):\n                up.append(\n                    UpBlock(\n                        in_channels,\n                        out_channels,\n                        has_attn=is_attn[i],\n                        activation=activation,\n                        norm=norm,\n                    )\n                )\n            # Final block to reduce the number of channels\n            out_channels = in_channels // ch_mults[i]\n            up.append(UpBlock(in_channels, out_channels, has_attn=is_attn[i], activation=activation, norm=norm))\n            in_channels = out_channels\n            # Up sample at all resolutions except last\n            if i &gt; 0:\n                up.append(Upsample(in_channels))\n\n        # Combine the set of modules\n        self.up = nn.ModuleList(up)\n\n        if norm:\n            self.norm = nn.GroupNorm(8, n_channels)\n        else:\n            self.norm = nn.Identity()\n        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n        #\n        if use1x1:\n            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        else:\n            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n\n    def forward(self, x: torch.Tensor):\n        assert x.dim() == 5\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n        x = self.image_proj(x)\n\n        h = [x]\n        for m in self.down:\n            x = m(x)\n            h.append(x)\n\n        x = self.middle(x)\n\n        for m in self.up:\n            if isinstance(m, Upsample):\n                x = m(x)\n            else:\n                # Get the skip connection from first half of U-Net and concatenate\n                s = h.pop()\n                x = torch.cat((x, s), dim=1)\n                #\n                x = m(x)\n\n        x = self.final(self.activation(self.norm(x)))\n        x = x.reshape(\n            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n        )\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.UpBlock","title":"<code>UpBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Up block that combines <code>ResidualBlock</code> and <code>AttentionBlock</code>.</p> <p>These are used in the second half of U-Net at each resolution.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>has_attn</code> <code>bool</code> <p>Whether to use attention block</p> <code>False</code> <code>activation</code> <code>str</code> <p>Activation function</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use normalization</p> <code>False</code> Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class UpBlock(nn.Module):\n    \"\"\"Up block that combines [`ResidualBlock`][pdearena.modules.twod_unet.ResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n\n    These are used in the second half of U-Net at each resolution.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        has_attn (bool): Whether to use attention block\n        activation (str): Activation function\n        norm (bool): Whether to use normalization\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        has_attn: bool = False,\n        activation: str = \"gelu\",\n        norm: bool = False,\n    ):\n        super().__init__()\n        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n        # from the first half of the U-Net\n        self.res = ResidualBlock(in_channels + out_channels, out_channels, activation=activation, norm=norm)\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        x = self.res(x)\n        x = self.attn(x)\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.twod_unet.Upsample","title":"<code>Upsample</code>","text":"<p>             Bases: <code>Module</code></p> <p>Scale up the feature map by \\(2 \\times\\)</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of channels in the input and output.</p> required Source code in <code>pdearena/modules/twod_unet.py</code> <pre><code>class Upsample(nn.Module):\n    r\"\"\"Scale up the feature map by $2 \\times$\n\n    Args:\n        n_channels (int): Number of channels in the input and output.\n    \"\"\"\n\n    def __init__(self, n_channels: int):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor):\n        return self.conv(x)\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.CliffordMaxwellResNet3D","title":"<code>CliffordMaxwellResNet3D</code>","text":"<p>             Bases: <code>Module</code></p> <p>3D building block for Clifford architectures with ResNet backbone network. The backbone networks follows these three steps:     1. Clifford vector+bivector encoding.     2. Basic blocks as provided.     3. Clifford vector+bivector decoding.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Union[tuple, list, Tensor]</code> <p>Signature of Clifford algebra.</p> required <code>block</code> <code>Module</code> <p>Choice of basic blocks.</p> required <code>num_blocks</code> <code>list</code> <p>List of basic blocks in each residual block.</p> required <code>time_history</code> <code>int</code> <p>Number of input timesteps.</p> required <code>time_future</code> <code>int</code> <p>Number of output timesteps.</p> required <code>hidden_channels</code> <code>int</code> <p>Number of hidden channels.</p> required <code>activation</code> <code>Callable</code> <p>Activation function. Defaults to F.gelu.</p> <code>'gelu'</code> <code>norm</code> <code>bool</code> <p>Whether to use Clifford (group) normalization. Defaults to False.</p> <code>False</code> <code>num_groups</code> <code>int</code> <p>Number of groups when using Clifford (group) normalization. Defaults to 1.</p> <code>1</code> Source code in <code>pdearena/modules/threed.py</code> <pre><code>class CliffordMaxwellResNet3D(nn.Module):\n    \"\"\"3D building block for Clifford architectures with ResNet backbone network.\n    The backbone networks follows these three steps:\n        1. Clifford vector+bivector encoding.\n        2. Basic blocks as provided.\n        3. Clifford vector+bivector decoding.\n\n    Args:\n        g (Union[tuple, list, torch.Tensor]): Signature of Clifford algebra.\n        block (nn.Module): Choice of basic blocks.\n        num_blocks (list): List of basic blocks in each residual block.\n        time_history (int): Number of input timesteps.\n        time_future (int): Number of output timesteps.\n        hidden_channels (int): Number of hidden channels.\n        activation (Callable, optional): Activation function. Defaults to F.gelu.\n        norm (bool, optional): Whether to use Clifford (group) normalization. Defaults to False.\n        num_groups (int, optional): Number of groups when using Clifford (group) normalization. Defaults to 1.\n    \"\"\"\n\n    # For periodic boundary conditions, set padding = 0.\n    padding = 2\n\n    def __init__(\n        self,\n        g: Union[tuple, list, torch.Tensor],\n        block: nn.Module,\n        num_blocks: list,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int,\n        activation: str = \"gelu\",\n        norm: bool = False,\n        num_groups: int = 1,\n        diffmode: bool = False,\n    ):\n        super().__init__()\n\n        # Encoding and decoding layers.\n        self.encoder = CliffordConv3dMaxwellEncoder(\n            g,\n            in_channels=time_history,\n            out_channels=hidden_channels,\n            kernel_size=1,\n            padding=0,\n        )\n        self.decoder = CliffordConv3dMaxwellDecoder(\n            g,\n            in_channels=hidden_channels,\n            out_channels=time_future,\n            kernel_size=1,\n            padding=0,\n        )\n\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        # Residual blocks.\n        self.layers = nn.ModuleList(\n            [\n                self._make_basic_block(\n                    g,\n                    block,\n                    hidden_channels,\n                    num_blocks[i],\n                    activation=self.activation,\n                    norm=norm,\n                    num_groups=num_groups,\n                )\n                for i in range(len(num_blocks))\n            ]\n        )\n\n    def _make_basic_block(\n        self,\n        g,\n        block: nn.Module,\n        hidden_channels: int,\n        num_blocks: int,\n        activation: Callable,\n        norm: bool,\n        num_groups: int,\n    ) -&gt; nn.Sequential:\n        blocks = []\n        for _ in range(num_blocks):\n            blocks.append(\n                block(\n                    g,\n                    hidden_channels,\n                    hidden_channels,\n                    activation=activation,\n                    norm=norm,\n                    num_groups=num_groups,\n                )\n            )\n        return nn.Sequential(*blocks)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        assert x.dim() == 6\n\n        # Get data into shape where I dimension is last.\n        B_dim, C_dim, I_dim, *D_dims = range(len(x.shape))\n        x = x.permute(B_dim, C_dim, *D_dims, I_dim)\n\n        # Encoding layer.\n        x = self.encoder(self.activation(x))\n\n        # Embed for non-periodic boundaries.\n        if self.padding &gt; 0:\n            B_dim, C_dim, *D_dims, I_dim = range(len(x.shape))\n            x = x.permute(B_dim, I_dim, C_dim, *D_dims)\n            x = F.pad(x, [0, self.padding, 0, self.padding, 0, self.padding])\n            B_dim, I_dim, C_dim, *D_dims = range(len(x.shape))\n            x = x.permute(B_dim, C_dim, *D_dims, I_dim)\n\n        # Apply residual layers.\n        for layer in self.layers:\n            x = layer(x)\n\n        # Decoding layer.\n        if self.padding &gt; 0:\n            B_dim, C_dim, *D_dims, I_dim = range(len(x.shape))\n            x = x.permute(B_dim, I_dim, C_dim, *D_dims)\n            x = x[..., : -self.padding, : -self.padding, : -self.padding]\n            B_dim, I_dim, C_dim, *D_dims = range(len(x.shape))\n            x = x.permute(B_dim, C_dim, *D_dims, I_dim)\n\n        # Output layer.\n        x = self.decoder(x)\n\n        # Get data back to normal shape.\n        B_dim, C_dim, *D_dims, I_dim = range(len(x.shape))\n        x = x.permute(B_dim, C_dim, I_dim, *D_dims)\n\n        return x\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.FourierBasicBlock3D","title":"<code>FourierBasicBlock3D</code>","text":"<p>             Bases: <code>Module</code></p> <p>Basic 3d FNO ResNet building block consisting of two 3d convolutional layers, two 3d SpectralConv layers and skip connections.</p> Source code in <code>pdearena/modules/threed.py</code> <pre><code>class FourierBasicBlock3D(nn.Module):\n    \"\"\"Basic 3d FNO ResNet building block consisting of two 3d convolutional layers,\n    two 3d SpectralConv layers and skip connections.\"\"\"\n\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_planes: int,\n        planes: int,\n        modes1: int,\n        modes2: int,\n        modes3: int,\n        stride: int = 1,\n        activation: str = \"gelu\",\n        norm: bool = False,\n    ):\n        \"\"\"Initialize basic 3d FNO ResNet building block\n        Args:\n            in_planes (int): Input channels\n            planes (int): Output channels\n            modes1 (int): Fourier modes for x direction.\n            modes2 (int): Fourier modes for y direction.\n            modes3 (int): Fourier modes for z direction.\n            stride (int, optional): stride of 2d convolution. Defaults to 1.\n            norm (bool): Wether to use normalization. Defaults to False.\n        \"\"\"\n        super().__init__()\n\n        self.fourier1 = SpectralConv3d(in_planes, planes, modes1=modes1, modes2=modes2, modes3=modes3)\n        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=1, padding=0, padding_mode=\"zeros\")\n        self.fourier2 = SpectralConv3d(planes, planes, modes1=modes1, modes2=modes2, modes3=modes3)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=1, padding=0, padding_mode=\"zeros\")\n\n        # Shortcut connection, batchnorm removed\n        # So far shortcut connections are not helping\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1))\n        if activation == \"gelu\":\n            self.activation = F.gelu\n        elif activation == \"relu\":\n            self.activation = F.relu\n        else:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        if norm:\n            raise NotImplementedError(f\"Normalization for FourierBasicBlock3D not implemented\")\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of basic 3d Fourier ResNet building block.\n        Args:\n            x (torch.Tensor): input of shape [batch, in_planes, x, y, z]\n        Returns:\n            torch.Tensor: output of shape [batch, planes, x, y, z]\n        \"\"\"\n        x1 = self.fourier1(x)\n        x2 = self.conv1(x)\n        out = self.activation(x1 + x2)\n\n        x1 = self.fourier2(out)\n        x2 = self.conv2(out)\n        out = x1 + x2\n        # out += self.shortcut(x)\n        out = self.activation(out)\n        return out\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.FourierBasicBlock3D.__init__","title":"<code>__init__(in_planes, planes, modes1, modes2, modes3, stride=1, activation='gelu', norm=False)</code>","text":"<p>Initialize basic 3d FNO ResNet building block Args:     in_planes (int): Input channels     planes (int): Output channels     modes1 (int): Fourier modes for x direction.     modes2 (int): Fourier modes for y direction.     modes3 (int): Fourier modes for z direction.     stride (int, optional): stride of 2d convolution. Defaults to 1.     norm (bool): Wether to use normalization. Defaults to False.</p> Source code in <code>pdearena/modules/threed.py</code> <pre><code>def __init__(\n    self,\n    in_planes: int,\n    planes: int,\n    modes1: int,\n    modes2: int,\n    modes3: int,\n    stride: int = 1,\n    activation: str = \"gelu\",\n    norm: bool = False,\n):\n    \"\"\"Initialize basic 3d FNO ResNet building block\n    Args:\n        in_planes (int): Input channels\n        planes (int): Output channels\n        modes1 (int): Fourier modes for x direction.\n        modes2 (int): Fourier modes for y direction.\n        modes3 (int): Fourier modes for z direction.\n        stride (int, optional): stride of 2d convolution. Defaults to 1.\n        norm (bool): Wether to use normalization. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.fourier1 = SpectralConv3d(in_planes, planes, modes1=modes1, modes2=modes2, modes3=modes3)\n    self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=1, padding=0, padding_mode=\"zeros\")\n    self.fourier2 = SpectralConv3d(planes, planes, modes1=modes1, modes2=modes2, modes3=modes3)\n    self.conv2 = nn.Conv3d(planes, planes, kernel_size=1, padding=0, padding_mode=\"zeros\")\n\n    # Shortcut connection, batchnorm removed\n    # So far shortcut connections are not helping\n    self.shortcut = nn.Sequential()\n    if stride != 1 or in_planes != self.expansion * planes:\n        self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1))\n    if activation == \"gelu\":\n        self.activation = F.gelu\n    elif activation == \"relu\":\n        self.activation = F.relu\n    else:\n        raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n    if norm:\n        raise NotImplementedError(f\"Normalization for FourierBasicBlock3D not implemented\")\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.FourierBasicBlock3D.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of basic 3d Fourier ResNet building block. Args:     x (torch.Tensor): input of shape [batch, in_planes, x, y, z] Returns:     torch.Tensor: output of shape [batch, planes, x, y, z]</p> Source code in <code>pdearena/modules/threed.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of basic 3d Fourier ResNet building block.\n    Args:\n        x (torch.Tensor): input of shape [batch, in_planes, x, y, z]\n    Returns:\n        torch.Tensor: output of shape [batch, planes, x, y, z]\n    \"\"\"\n    x1 = self.fourier1(x)\n    x2 = self.conv1(x)\n    out = self.activation(x1 + x2)\n\n    x1 = self.fourier2(out)\n    x2 = self.conv2(out)\n    out = x1 + x2\n    # out += self.shortcut(x)\n    out = self.activation(out)\n    return out\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.MaxwellResNet3D","title":"<code>MaxwellResNet3D</code>","text":"<p>             Bases: <code>Module</code></p> <p>3d ResNet model for Maxwell equations, difference to default ResNet architectures is that spatial resolution and channels (in_planes) stay constant throughout the network.</p> Source code in <code>pdearena/modules/threed.py</code> <pre><code>class MaxwellResNet3D(nn.Module):\n    \"\"\"3d ResNet model for Maxwell equations, difference to default ResNet architectures is that\n    spatial resolution and channels (in_planes) stay constant throughout the network.\"\"\"\n\n    padding = 2  # no periodic\n\n    def __init__(\n        self,\n        block: nn.Module,\n        num_blocks: list,\n        time_history: int,\n        time_future: int,\n        hidden_channels: int = 64,\n        activation: str = \"gelu\",\n        diffmode: bool = False,\n    ):\n        \"\"\"Initialize 3d ResNet model\n\n        Args:\n            block (nn.Module): basic 3d ResNet building block\n            num_blocks (list): list of basic building blocks per layer\n            time_history (int): input timesteps\n            time_future (int): prediction timesteps\n            hidden_channels (int): hidden channels in the ResNet blocks\n        \"\"\"\n        super().__init__()\n\n        self.diffmode = diffmode\n        self.in_planes = hidden_channels\n        self.conv_in1 = nn.Conv3d(\n            time_history * 6,\n            self.in_planes,\n            kernel_size=1,\n            bias=True,\n        )\n        self.conv_in2 = nn.Conv3d(\n            self.in_planes,\n            self.in_planes,\n            kernel_size=1,\n            bias=True,\n        )\n        self.conv_out1 = nn.Conv3d(\n            self.in_planes,\n            self.in_planes,\n            kernel_size=1,\n            bias=True,\n        )\n        self.conv_out2 = nn.Conv3d(\n            self.in_planes,\n            time_future * 6,\n            kernel_size=1,\n            bias=True,\n        )\n\n        self.layers = nn.ModuleList(\n            [\n                self._make_layer(\n                    block,\n                    self.in_planes,\n                    num_blocks[i],\n                    stride=1,\n                    activation=activation,\n                )\n                for i in range(len(num_blocks))\n            ]\n        )\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n    def _make_layer(\n        self,\n        block: Callable,\n        planes: int,\n        num_blocks: int,\n        stride: int,\n        activation: str,\n    ) -&gt; nn.Sequential:\n        \"\"\"Build 3d ResNet layers out of basic building blocks.\n\n        Args:\n            block (nn.Module): basic 3d ResNet building block\n            planes (int): input channels\n            num_blocks (int): number of basic 3d ResNet building blocks in one layer\n            stride (int): stride\n\n        Returns:\n            nn.Sequential: 3d ResNet layer as nn.Sequential\n        \"\"\"\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(in_planes=self.in_planes, planes=planes, stride=stride, activation=activation))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def __repr__(self):\n        return \"ResNet\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the ResNet in 3 spatial dimensions\n        consisting of embedding layer(s), ResNet building blogs and output layer.\n        Args:\n            x (torch.Tensor): input tensor of shape [b, time_history, 6, x, y, z]\n        Returns:\n            torch.Tensor: output tensor of shape [b, time_future, 6, x, y, z]\n        \"\"\"\n        assert x.dim() == 6\n        orig_shape = x.shape\n        x = x.reshape(x.size(0), -1, *x.shape[3:])\n        prev = x\n        x = self.activation(self.conv_in1(x.float()))\n        x = self.activation(self.conv_in2(x.float()))\n\n        if self.padding &gt; 0:\n            x = F.pad(x, [0, self.padding, 0, self.padding])\n        for layer in self.layers:\n            x = layer(x)\n        if self.padding &gt; 0:\n            x = x[..., : -self.padding, : -self.padding]\n\n        x = self.conv_out1(x)\n        x = self.conv_out2(x)\n\n        if self.diffmode:\n            x = x + prev[:, -1:, ...].detach()\n        return x.reshape(orig_shape[0], -1, 6, *orig_shape[3:])\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.MaxwellResNet3D.__init__","title":"<code>__init__(block, num_blocks, time_history, time_future, hidden_channels=64, activation='gelu', diffmode=False)</code>","text":"<p>Initialize 3d ResNet model</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>Module</code> <p>basic 3d ResNet building block</p> required <code>num_blocks</code> <code>list</code> <p>list of basic building blocks per layer</p> required <code>time_history</code> <code>int</code> <p>input timesteps</p> required <code>time_future</code> <code>int</code> <p>prediction timesteps</p> required <code>hidden_channels</code> <code>int</code> <p>hidden channels in the ResNet blocks</p> <code>64</code> Source code in <code>pdearena/modules/threed.py</code> <pre><code>def __init__(\n    self,\n    block: nn.Module,\n    num_blocks: list,\n    time_history: int,\n    time_future: int,\n    hidden_channels: int = 64,\n    activation: str = \"gelu\",\n    diffmode: bool = False,\n):\n    \"\"\"Initialize 3d ResNet model\n\n    Args:\n        block (nn.Module): basic 3d ResNet building block\n        num_blocks (list): list of basic building blocks per layer\n        time_history (int): input timesteps\n        time_future (int): prediction timesteps\n        hidden_channels (int): hidden channels in the ResNet blocks\n    \"\"\"\n    super().__init__()\n\n    self.diffmode = diffmode\n    self.in_planes = hidden_channels\n    self.conv_in1 = nn.Conv3d(\n        time_history * 6,\n        self.in_planes,\n        kernel_size=1,\n        bias=True,\n    )\n    self.conv_in2 = nn.Conv3d(\n        self.in_planes,\n        self.in_planes,\n        kernel_size=1,\n        bias=True,\n    )\n    self.conv_out1 = nn.Conv3d(\n        self.in_planes,\n        self.in_planes,\n        kernel_size=1,\n        bias=True,\n    )\n    self.conv_out2 = nn.Conv3d(\n        self.in_planes,\n        time_future * 6,\n        kernel_size=1,\n        bias=True,\n    )\n\n    self.layers = nn.ModuleList(\n        [\n            self._make_layer(\n                block,\n                self.in_planes,\n                num_blocks[i],\n                stride=1,\n                activation=activation,\n            )\n            for i in range(len(num_blocks))\n        ]\n    )\n    self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n    if self.activation is None:\n        raise NotImplementedError(f\"Activation {activation} not implemented\")\n</code></pre>"},{"location":"reference/modules/#pdearena.modules.threed.MaxwellResNet3D.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the ResNet in 3 spatial dimensions consisting of embedding layer(s), ResNet building blogs and output layer. Args:     x (torch.Tensor): input tensor of shape [b, time_history, 6, x, y, z] Returns:     torch.Tensor: output tensor of shape [b, time_future, 6, x, y, z]</p> Source code in <code>pdearena/modules/threed.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the ResNet in 3 spatial dimensions\n    consisting of embedding layer(s), ResNet building blogs and output layer.\n    Args:\n        x (torch.Tensor): input tensor of shape [b, time_history, 6, x, y, z]\n    Returns:\n        torch.Tensor: output tensor of shape [b, time_future, 6, x, y, z]\n    \"\"\"\n    assert x.dim() == 6\n    orig_shape = x.shape\n    x = x.reshape(x.size(0), -1, *x.shape[3:])\n    prev = x\n    x = self.activation(self.conv_in1(x.float()))\n    x = self.activation(self.conv_in2(x.float()))\n\n    if self.padding &gt; 0:\n        x = F.pad(x, [0, self.padding, 0, self.padding])\n    for layer in self.layers:\n        x = layer(x)\n    if self.padding &gt; 0:\n        x = x[..., : -self.padding, : -self.padding]\n\n    x = self.conv_out1(x)\n    x = self.conv_out2(x)\n\n    if self.diffmode:\n        x = x + prev[:, -1:, ...].detach()\n    return x.reshape(orig_shape[0], -1, 6, *orig_shape[3:])\n</code></pre>"},{"location":"reference/visualization/","title":"Visualization","text":""},{"location":"reference/visualization/#pdearena.visualization.plot_2dvec","title":"<code>plot_2dvec(ax, vec)</code>","text":"<p>Plot a 2D vector field.</p> <p>Cleanups up the axes and returns the quiver object.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axes to plot on.</p> required <code>vec</code> <code>ndarray</code> <p>[2, ...] 2D Vector field to plot.</p> required <p>Returns:</p> Type Description <code>Quiver</code> <p>Quiver object.</p> Source code in <code>pdearena/visualization.py</code> <pre><code>def plot_2dvec(ax, vec: np.ndarray) -&gt; mpl.quiver.Quiver:\n    \"\"\"Plot a 2D vector field.\n\n    Cleanups up the axes and returns the quiver object.\n\n    Args:\n        ax (mpl.axes.Axes): Axes to plot on.\n        vec (np.ndarray): [2, ...] 2D Vector field to plot.\n\n    Returns:\n        (mpl.quiver.Quiver): Quiver object.\n    \"\"\"\n    qv = ax.quiver(vec[0, ...], vec[1, ...])\n    cleanup_axes(ax)\n    return qv\n</code></pre>"},{"location":"reference/visualization/#pdearena.visualization.plot_scalar","title":"<code>plot_scalar(ax, scalar)</code>","text":"<p>Plot a scalar field.</p> <p>Cleanups up the axes and returns the image object.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axes to plot on.</p> required <code>scalar</code> <code>ndarray</code> <p>2D Scalar field to plot.</p> required <p>Returns:</p> Type Description <code>AxesImage</code> <p>Image object.</p> Source code in <code>pdearena/visualization.py</code> <pre><code>def plot_scalar(ax, scalar: np.ndarray) -&gt; mpl.image.AxesImage:\n    \"\"\"Plot a scalar field.\n\n    Cleanups up the axes and returns the image object.\n\n    Args:\n        ax (mpl.axes.Axes): Axes to plot on.\n        scalar (np.ndarray): 2D Scalar field to plot.\n\n    Returns:\n        (mpl.image.AxesImage): Image object.\n    \"\"\"\n    im = ax.imshow(scalar)\n    cleanup_axes(ax)\n    return im\n</code></pre>"},{"location":"reference/visualization/#pdearena.visualization.plot_scalar_sequence_comparison","title":"<code>plot_scalar_sequence_comparison(init_field, ground_truth, prediction, fontsize=37, text_loc=(-10, 64))</code>","text":"<p>Plot a scalar field sequence comparison.</p> <p>Parameters:</p> Name Type Description Default <code>init_field</code> <code>ndarray</code> <p>Initial scalar field. We only plot the last time step of the initial field.</p> required <code>ground_truth</code> <code>ndarray</code> <p>Ground truth scalar field.</p> required <code>prediction</code> <code>ndarray</code> <p>Predicted scalar field.</p> required <code>fontsize</code> <code>int</code> <p>Fontsize for the text annotations. Defaults to 37.</p> <code>37</code> <code>text_loc</code> <code>tuple</code> <p>Location of the text annotations. Defaults to (-10, 64).</p> <code>(-10, 64)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure object.</p> Source code in <code>pdearena/visualization.py</code> <pre><code>def plot_scalar_sequence_comparison(init_field, ground_truth, prediction, fontsize=37, text_loc=(-10, 64)):\n    \"\"\"Plot a scalar field sequence comparison.\n\n    Args:\n        init_field (np.ndarray): Initial scalar field. We only plot the last time step of the initial field.\n        ground_truth (np.ndarray): Ground truth scalar field.\n        prediction (np.ndarray): Predicted scalar field.\n        fontsize (int, optional): Fontsize for the text annotations. Defaults to 37.\n        text_loc (tuple, optional): Location of the text annotations. Defaults to (-10, 64).\n\n    Returns:\n        (mpl.figure.Figure): Figure object.\n    \"\"\"\n    assert ground_truth.shape == prediction.shape\n    err = np.abs(ground_truth - prediction)\n    n_timesteps = ground_truth.shape[0]\n    scaling = max(ground_truth.shape[-1] / ground_truth.shape[-2], 1)\n    fig = plt.figure(figsize=(n_timesteps * 6 * scaling, 15))\n\n    # Plot the last timestep of init_field\n    ## create space for init_field\n    gs = GridSpec(3, n_timesteps + 1, figure=fig)\n    ax1 = fig.add_subplot(gs[0, 0])\n    im1 = plot_scalar(ax1, np.transpose(init_field[-1, ...], (1, 2, 0)))\n    ax1.text(*text_loc, s=\"Ground truth\", fontdict={\"ha\": \"center\", \"va\": \"center\"}, rotation=90, fontsize=fontsize)\n\n    # Plot the ground truth sequence\n    for i in range(n_timesteps):\n        ax = fig.add_subplot(gs[0, i + 1])\n        im1 = plot_scalar(ax, np.transpose(ground_truth[i, ...], (1, 2, 0)))\n\n    # Plot the prediction sequence\n    for i in range(n_timesteps):\n        ax = fig.add_subplot(gs[1, i + 1])\n        im2 = plot_scalar(ax, np.transpose(prediction[i, ...], (1, 2, 0)))\n        if i == 0:\n            ax.text(\n                *text_loc, s=\"Prediction\", fontdict={\"ha\": \"center\", \"va\": \"center\"}, rotation=90, fontsize=fontsize\n            )\n\n    # Plot the error sequence\n    for i in range(n_timesteps):\n        ax = fig.add_subplot(gs[2, i + 1])\n        im3 = plot_scalar(ax, np.transpose(err[i, ...], (1, 2, 0)))\n        if i == 0:\n            ax.text(*text_loc, s=\"Error\", fontdict={\"ha\": \"center\", \"va\": \"center\"}, rotation=90, fontsize=fontsize)\n\n    # Plot the colorbars\n    ## Create space for colorbars\n    fig.subplots_adjust(wspace=0, hspace=0, right=0.9)\n    ## Add colorbars\n    cax1 = fig.add_axes([0.9, 0.15, 0.02, 0.2])\n    fig.colorbar(im3, cax=cax1)\n    cax1.tick_params(labelsize=fontsize * 0.6)\n    cax2 = fig.add_axes([0.9, 0.4, 0.02, 0.2])\n    fig.colorbar(im2, cax=cax2)\n    cax2.tick_params(labelsize=fontsize * 0.6)\n\n    return fig\n</code></pre>"},{"location":"scenarios/applied/","title":"Scenario: You have a new class of PDE and want to see if PDE surrogates can be useful for you","text":"<p>There are hundreds of possible PDE surrogate models. As with most deep learning, choice of hyperparameters can matter a lot too. You might also have your own runtime requirements, and data or time limitations. PDEArena provides a simple interface for you to try out many possible model designs and understand the best model for your task and constraints. All you need to do is, define your PDE configuration details in <code>pde.py</code>, write a <code>IterDataPipe</code> for your dataset, add it to the <code>DATAPIPE_REGISTRY</code>, and you should be good to go.</p>"},{"location":"scenarios/applied/#simple-example","title":"Simple Example","text":"<p>Let's say you want a new task modeling Shallow water making predictions at 18 hours interval. You can subclass the <code>ShallowWaterDatasetOpener</code> as:</p> <pre><code>class ShallowWaterDatasetOpener18Hr(ShallowWaterDatasetOpener):\n    def __init__(\n        self,\n        dp,\n        mode: str,\n        limit_trajectories: Optional[int] = None,\n        usegrid: bool = False,\n    ) -&gt; None:\n        # sample_rate=1 implies 6hr\n        super().__init__(dp, mode, limit_trajectories, usevort=False, usegrid=usegrid, sample_rate=3)\n</code></pre> <p>Now you can set up the various datapipes for training, validation and testing:</p> <pre><code># Train\ntrain_datapipe_18Hr_vel = functools.partial(\n    build_datapipes,\n    dataset_opener=ShallowWaterDatasetOpener18Hr,\n    lister=ZarrLister,\n    filter_fn=_weathertrain_filter,\n    sharder=_sharder,\n    mode=\"train\",\n)\n# Valid\nonestep_valid_datapipe_18Hr_vel = functools.partial(\n    build_datapipes,\n    dataset_opener=ShallowWaterDatasetOpener18Hr,\n    lister=ZarrLister,\n    filter_fn=_weathervalid_filter,\n    sharder=_sharder,\n    mode=\"valid\",\n    onestep=True,\n)\ntrajectory_valid_datapipe_18Hr_vel = functools.partial(\n    build_datapipes,\n    dataset_opener=ShallowWaterDatasetOpener18Hr,\n    lister=ZarrLister,\n    filter_fn=_weathervalid_filter,\n    sharder=_sharder,\n    mode=\"valid\",\n    onestep=False,\n)\n# Test\nonestep_test_datapipe_18Hr_vel = functools.partial(\n    build_datapipes,\n    dataset_opener=ShallowWaterDatasetOpener18Hr,\n    lister=ZarrLister,\n    filter_fn=_weathertest_filter,\n    sharder=_sharder,\n    mode=\"test\",\n    onestep=True,\n)\ntrajectory_test_datapipe_18Hr_vel = functools.partial(\n    build_datapipes,\n    dataset_opener=ShallowWaterDatasetOpener18Hr,\n    lister=ZarrLister,\n    filter_fn=_weathertest_filter,\n    sharder=_sharder,\n    mode=\"test\",\n    onestep=False,\n)\n</code></pre> <p>Then you can add these datapipes to the data <code>registry</code>: <pre><code>DATAPIPE_REGISTRY[\"ShallowWater2DVel-18Hr\"] = {}\nDATAPIPE_REGISTRY[\"ShallowWater2DVel-18Hr\"][\"train\"] = train_datapipe_18Hr_vel\nDATAPIPE_REGISTRY[\"ShallowWater2DVel-18Hr\"][\"valid\"] = [\n    onestep_valid_datapipe_18Hr_vel,\n    trajectory_valid_datapipe_18Hr_vel,\n]\nDATAPIPE_REGISTRY[\"ShallowWater2DVel-18Hr\"][\"test\"] = [\n    onestep_test_datapipe_18Hr_vel,\n    trajectory_test_datapipe_18Hr_vel,\n]\n</code></pre></p> <p>Finally you can train different models from the model zoo by setting the <code>data.task=ShallowWater2DVel-18Hr</code>: <pre><code>data:\n  task: ShallowWater2DVel-18Hr\n</code></pre> See <code>config</code> for an example of training with 2-day prediction.</p>"},{"location":"scenarios/beginner/","title":"Scenario: You are a beginner to PDE surrogate modeling","text":"<p>PDE surrogate modeling has seen quite some interest being at the intersection of AI and Science. You probably have read an exciting PDE surrogate modeling paper (more frequently termed operator learning paper), and maybe you want to understand how the pipeline works. PDEArena can be a perfect place for you to start learning about standardized PDE surrogate implementations and evaluations, following modern deep learning best practices.</p>"},{"location":"scenarios/researcher/","title":"Scenario: You are a PDE + ML researcher","text":""},{"location":"scenarios/researcher/#simple-example","title":"Simple Example","text":"<p>Say you want to evaluate a conditioned version of <code>DilatedResNet</code>. You can add a version of <code>DilatedBasicBlock</code> that works with an embedding vector as follows:</p> <pre><code>class CondDilatedBasicBlock(nn.Module):\n    \"\"\"Basic block for Dilated ResNet\n\n    Args:\n        in_planes (int): number of input channels\n        planes (int): number of output channels\n        stride (int, optional): stride of the convolution. Defaults to 1.\n        activation (str, optional): activation function. Defaults to \"relu\".\n        norm (bool, optional): whether to use group normalization. Defaults to True.\n        num_groups (int, optional): number of groups for group normalization. Defaults to 1.\n    \"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self,\n        in_planes: int,\n        planes: int,\n        cond_channels: int,\n        stride: int = 1,\n        activation: str = \"relu\",\n        norm: bool = True,\n        num_groups: int = 1,\n    ):\n        super().__init__()\n\n        self.dilation = [1, 2, 4, 8, 4, 2, 1]\n        dilation_layers = []\n        for dil in self.dilation:\n            dilation_layers.append(\n                nn.Conv2d(\n                    in_planes,\n                    planes,\n                    kernel_size=3,\n                    stride=stride,\n                    dilation=dil,\n                    padding=dil,\n                    bias=True,\n                )\n            )\n        self.dilation_layers = nn.ModuleList(dilation_layers)\n        self.norm_layers = nn.ModuleList(\n            nn.GroupNorm(num_groups, num_channels=planes) if norm else nn.Identity() for dil in self.dilation\n        )\n        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n        if self.activation is None:\n            raise NotImplementedError(f\"Activation {activation} not implemented\")\n\n        self.cond_emb = nn.Linear(cond_channels, planes)\n\n    def forward(self, x: torch.Tensor, emb: torch.Tensor) -&gt; torch.Tensor:\n        out = x\n        emb_out = self.cond_emb(emb)\n        while len(emb_out.shape) &lt; len(x.shape):\n            emb_out = emb_out[..., None]\n\n        for layer, norm in zip(self.dilation_layers, self.norm_layers):\n            out = self.activation(layer(norm(out)))\n        return out + x + emb_out\n</code></pre> <p>Now we can add an appropriate instantiation of the model in the <code>COND_MODEL_REGISTRY</code>:</p> <pre><code>COND_MODEL_REGISTRY[\"DilResNet-128\"] = {\n    \"class_path\": \"pdearena.modules.conditioned.twod_resnet.ResNet\",\n    \"init_args\": {\n        \"hidden_channels\": 128,\n        \"norm\": True,\n        \"num_blocks\": [1, 1, 1, 1],\n        \"block\": CondDilatedBasicBlock\n    }\n}\n</code></pre> <p>Finally you can train this model by appropriately setting up <code>model.name=DilResNet-128</code> in the training config: <pre><code>model:\n    name: DilResNet-128\n    max_num_steps: 5\n    activation: \"gelu\"\n    criterion: mse\n    lr: 1e-3\n    param_conditioning: \"scalar\"\n</code></pre></p>"}]}